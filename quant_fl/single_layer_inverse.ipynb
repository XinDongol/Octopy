{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#GPU:  4\n",
      "PyTorch Version: 1.6.0+cu92\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from utils import progress_bar\n",
    "\n",
    "\n",
    "from pprint import pprint as pp\n",
    "from scipy import linalg\n",
    "\n",
    "from utils import FlowLayout\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('#GPU: ', torch.cuda.device_count())\n",
    "print('PyTorch Version:', torch.__version__)\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 0, 0],\n",
       "       [0, 0, 1, 2]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_roll_matrix(r, nrow, stride):\n",
    "    r = np.array(r).flatten()\n",
    "    res = []\n",
    "    for i in range(nrow):\n",
    "        res.append(np.roll(r, stride*i))\n",
    "        \n",
    "    return np.stack(res)\n",
    "\n",
    "get_roll_matrix([1,2,0,0], 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toeplitz_1_ch(kernel, input_size, stride):\n",
    "    # shapes\n",
    "    k_h, k_w = kernel.shape\n",
    "    i_h, i_w = input_size\n",
    "    o_h, o_w = int((i_h-k_h)/stride) + 1, int((i_w-k_w)/stride) + 1\n",
    "\n",
    "    # construct 1d conv toeplitz matrices for each row of the kernel\n",
    "    toeplitz = []\n",
    "    for r in range(k_h):\n",
    "        toeplitz.append( get_roll_matrix((*kernel[r], *np.zeros(i_w-k_w)), o_h, stride) ) \n",
    "    # print(toeplitz)\n",
    "\n",
    "    # construct toeplitz matrix of toeplitz matrices (just for padding=0)\n",
    "    h_blocks, w_blocks = o_h, i_h\n",
    "    h_block, w_block = toeplitz[0].shape\n",
    "\n",
    "    # 2, 2, 4, 4\n",
    "    W_conv = np.zeros((h_blocks, h_block, w_blocks, w_block))\n",
    "\n",
    "    for i, B in enumerate(toeplitz):\n",
    "        for j in range(o_h):\n",
    "            W_conv[j, :, i+j*2, :] = B\n",
    "\n",
    "    # print(W_conv.shape)\n",
    "    # print(W_conv)\n",
    "    W_conv.shape = (h_blocks*h_block, w_blocks*w_block)\n",
    "\n",
    "    return W_conv\n",
    "\n",
    "\n",
    "def toeplitz_mult_ch(kernel, input_size, stride):\n",
    "    \"\"\"Compute toeplitz matrix for 2d conv with multiple in and out channels.\n",
    "    Args:\n",
    "        kernel: shape=(n_out, n_in, H_k, W_k)\n",
    "        input_size: (n_in, H_i, W_i)\"\"\"\n",
    "\n",
    "    kernel_size = kernel.shape\n",
    "    output_size = (kernel_size[0], int((input_size[1] - kernel_size[2])/stride)+1, int((input_size[2] - kernel_size[3])/stride)+1)\n",
    "    print('==> output_size', output_size)\n",
    "    T = np.zeros((output_size[0], int(np.prod(output_size[1:])), input_size[0], int(np.prod(input_size[1:]))))\n",
    "    print('==> T', T.shape)\n",
    "\n",
    "    for i,ks in enumerate(kernel):  # loop over output channel\n",
    "        for j,k in enumerate(ks):  # loop over input channel\n",
    "            T_k = toeplitz_1_ch(k, input_size[1:], stride)\n",
    "            T[i, :, j, :] = T_k\n",
    "\n",
    "    T.shape = (np.prod(output_size), np.prod(input_size))\n",
    "\n",
    "    return T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InvConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InvConv, self).__init__()\n",
    "        self.if_init = False\n",
    "\n",
    "    def init_layer(self, old_layer):\n",
    "        self.old_weight = old_layer.weight.detach().cpu().clone().numpy()\n",
    "        self.inp_padding = old_layer.padding\n",
    "        \n",
    "        self.inp_size = list(old_layer.inp_size)[1:] # inp_size was obtained by hook\n",
    "        \n",
    "        # add padding into the inp_size\n",
    "        if old_layer.padding[0]>0:\n",
    "            self.inp_size[-1] += 2*old_layer.padding[0]\n",
    "            self.inp_size[-2] += 2*old_layer.padding[0]\n",
    "        \n",
    "        self.stride = old_layer.stride[0]\n",
    "            \n",
    "        print('inp_size:', self.inp_size)\n",
    "        print('old weight', self.old_weight.shape)\n",
    "        self.old_weight_matrix = toeplitz_mult_ch(\n",
    "            self.old_weight, self.inp_size, self.stride)\n",
    "        self.old_weight_matrix = torch.tensor(self.old_weight_matrix).float()\n",
    "        \n",
    "        # self.inv_old_weight_matrix = self.old_weight_matrix.t()\n",
    "        self.inv_old_weight_matrix = torch.pinverse(self.old_weight_matrix)\n",
    "        \n",
    "        \n",
    "        self.if_init = True\n",
    "        \n",
    "    def forward(self, y, if_inv=True):\n",
    "        '''\n",
    "        Problem: \n",
    "            1. the converted weight matrix is super large. This matrix may consum ~150G memory. Please do NOT use cuda. \n",
    "        '''\n",
    "        \n",
    "        assert self.if_init\n",
    "\n",
    "        # remember to transpose the old_weight_matrix\n",
    "        self.old_weight_matrix = self.old_weight_matrix.to(y.device)\n",
    "            \n",
    "        if if_inv:\n",
    "            '''\n",
    "            inversion of convolution\n",
    "            '''\n",
    "            y = y.contiguous()\n",
    "            out = torch.matmul(self.inv_old_weight_matrix[None,:,:], y.view(y.size(0), -1)[:,:,None])\n",
    "\n",
    "            # reshape the output\n",
    "            out = out.view([out.size(0)] + self.inp_size)\n",
    "\n",
    "            # un-padding\n",
    "            if self.inp_padding[0] == 0:\n",
    "                out_unpadding = out\n",
    "            else:\n",
    "                out_unpadding = out[:,:,self.inp_padding[0]:-self.inp_padding[0],self.inp_padding[1]:-self.inp_padding[1]]\n",
    "\n",
    "            return out_unpadding\n",
    "        else:\n",
    "            '''\n",
    "            standard convolution. However, we implement the convolution with pure matrix(weights)-vector(input) multiplication.\n",
    "            \n",
    "            '''\n",
    "            x = y\n",
    "            x_pad = F.pad(x, pad=[self.inp_padding[0],self.inp_padding[0],self.inp_padding[1],self.inp_padding[1]])\n",
    "            print('### x_pad', x_pad.view(x_pad.size(0), -1)[:,:,None].size())\n",
    "            out = torch.matmul(self.old_weight_matrix[None,:,:], x_pad.view(x_pad.size(0), -1)[:,:,None])\n",
    "            # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv#torch.nn.Conv2d\n",
    "            out_size = int( (self.inp_size[-1] - 1 * (self.old_weight.shape[-1] -1) - 1)  / self.stride + 1)\n",
    "            out = out.view([out.size(0), self.old_weight.shape[0]]+[out_size]*2)\n",
    "            return out\n",
    "\n",
    "if 0:\n",
    "    inp = torch.randn(10,4,8,8)\n",
    "    gt = nn.Conv2d(in_channels=4, out_channels=2, kernel_size=2, stride=2, padding=1, bias=False)\n",
    "    gt.inp_size = inp.size()\n",
    "    diy = InvConv()\n",
    "    diy.init_layer(gt)\n",
    "    print((diy(inp, False)-gt(inp)).abs().max())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "    'VGG_stride': [64, 128, 256, 512, 512],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.block0 = self._make_layers(3, 64)\n",
    "        self.block1 = self._make_layers(64, 128)\n",
    "        self.block2 = self._make_layers(128, 256)\n",
    "        self.block3 = self._make_layers(256, 512)\n",
    "        self.block4 = self._make_layers(512, 512, False)\n",
    "        self.classifier = nn.Linear(512, 10, bias=False)\n",
    "\n",
    "    def forward(self, x, aux_output=False):\n",
    "        out_block0 = self.block0(x)\n",
    "        out_block1 = self.block1(out_block0)\n",
    "        out_block2 = self.block2(out_block1)\n",
    "        out_block3 = self.block3(out_block2)\n",
    "        out_block4 = self.block4(out_block3)\n",
    "        out_final  = self.classifier(out_block4.view(-1, 512))\n",
    "        if aux_output:\n",
    "            return out_final, [out_block4, out_block3, out_block2, out_block1, out_block0, x]\n",
    "        else:\n",
    "            return out_final\n",
    "\n",
    "\n",
    "    def _make_layers(self, in_channels, out_channels, wbn=True):\n",
    "        layers = []\n",
    "        layers += [nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)]\n",
    "        if wbn:\n",
    "            layers += [nn.BatchNorm2d(out_channels)]\n",
    "        layers += [nn.ReLU(inplace=False)]\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Preparing data..\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='/dev/shm', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=64, shuffle=True, num_workers=8)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='/dev/shm', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=64, shuffle=False, num_workers=8)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================================================>]  Step: 11ms | Tot: 2s524ms | Loss: 0.450 | Acc: 86.990% (8699/10000) 157/157 =============================================================>..]  Step: 15ms | Tot: 2s436ms | Loss: 0.451 | Acc: 87.024% (8410/9664) 151/157 ==============================================================>..]  Step: 15ms | Tot: 2s451ms | Loss: 0.451 | Acc: 87.037% (8467/9728) 152/157 \n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "lr = 0.01\n",
    "epochs = 100\n",
    "best_acc = 0\n",
    "\n",
    "net = VGG('VGG11').to(device)\n",
    "# if device == 'cuda':\n",
    "#     net = torch.nn.DataParallel(net)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# if args.resume:\n",
    "#     # Load checkpoint.\n",
    "#     print('==> Resuming from checkpoint..')\n",
    "#     assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "#     checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "#     net.load_state_dict(checkpoint['net'])\n",
    "#     best_acc = checkpoint['acc']\n",
    "#     start_epoch = checkpoint['epoch']\n",
    "start_epoch = 0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, eta_min=0, last_epoch=-1)\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        # state = {\n",
    "        #     'net': net.module.state_dict(),\n",
    "        #     'acc': acc,\n",
    "        #     'epoch': epoch,\n",
    "        # }\n",
    "        # torch.save(state, './cifar_vgg_11.pth')\n",
    "        best_acc = acc\n",
    "\n",
    "\n",
    "# for epoch in range(start_epoch, start_epoch+epochs):\n",
    "#     train(epoch)\n",
    "#     test(epoch)\n",
    "#     scheduler.step()\n",
    "\n",
    "checkpoint = torch.load('./cifar_vgg_stride.pth')\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "\n",
    "\n",
    "        \n",
    "def InpSizeHook(module, input, output):\n",
    "    module.inp_size = input[0].size()\n",
    "    \n",
    "for n,m in net.named_modules():\n",
    "    if n!='' and ('.' in n or 'classifier' in n):\n",
    "        m.register_forward_hook(InpSizeHook)\n",
    "    \n",
    "test(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block0.0 || Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) || torch.Size([16, 3, 32, 32])\n",
      "block0.1 || BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) || torch.Size([16, 64, 16, 16])\n",
      "block0.2 || ReLU() || torch.Size([16, 64, 16, 16])\n",
      "block1.0 || Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) || torch.Size([16, 64, 16, 16])\n",
      "block1.1 || BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) || torch.Size([16, 128, 8, 8])\n",
      "block1.2 || ReLU() || torch.Size([16, 128, 8, 8])\n",
      "block2.0 || Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) || torch.Size([16, 128, 8, 8])\n",
      "block2.1 || BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) || torch.Size([16, 256, 4, 4])\n",
      "block2.2 || ReLU() || torch.Size([16, 256, 4, 4])\n",
      "block3.0 || Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) || torch.Size([16, 256, 4, 4])\n",
      "block3.1 || BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) || torch.Size([16, 512, 2, 2])\n",
      "block3.2 || ReLU() || torch.Size([16, 512, 2, 2])\n",
      "block4.0 || Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) || torch.Size([16, 512, 2, 2])\n",
      "block4.1 || ReLU() || torch.Size([16, 512, 1, 1])\n",
      "classifier || Linear(in_features=512, out_features=10, bias=False) || torch.Size([16, 512])\n"
     ]
    }
   ],
   "source": [
    "for n,m in net.named_modules():\n",
    "    if n!='' and ('.' in n or 'classifier' in n):\n",
    "        print(n,'||',m, '||',m.inp_size)\n",
    "        # print()\n",
    "        # if '9' in n:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvVGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InvVGG, self).__init__()\n",
    "        self.classifier = nn.Linear(10, 512)\n",
    "        self.f40 = InvConv()\n",
    "        self.f31 = nn.BatchNorm2d(512)\n",
    "        self.f30 = InvConv()\n",
    "        self.f21 = nn.BatchNorm2d(256)\n",
    "        self.f20 = InvConv()\n",
    "        self.f11 = nn.BatchNorm2d(128)\n",
    "        self.f10 = InvConv()\n",
    "        self.f01 = nn.BatchNorm2d(64)\n",
    "        self.f00 = InvConv()\n",
    "        \n",
    "    def forward(self, out):\n",
    "        out = self.classifier(out)\n",
    "        # print('hehe', out.size())\n",
    "        out = out.view(out.size(0), 512, 1, 1)\n",
    "        out = self.f40(out)\n",
    "        out = self.f31(out)\n",
    "        out = self.f30(out)\n",
    "        out = self.f21(out)\n",
    "        out = self.f20(out)\n",
    "        out = self.f11(out)\n",
    "        out = self.f10(out)\n",
    "        out = self.f01(out)\n",
    "        out = self.f00(out)           \n",
    "        return out\n",
    "    \n",
    "inv_net = InvVGG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp_size: [512, 4, 4]\n",
      "old weight (512, 512, 3, 3)\n",
      "==> output_size (512, 1, 1)\n",
      "==> T (512, 1, 512, 16)\n",
      "inp_size: [256, 6, 6]\n",
      "old weight (512, 256, 3, 3)\n",
      "==> output_size (512, 2, 2)\n",
      "==> T (512, 4, 256, 36)\n",
      "inp_size: [128, 10, 10]\n",
      "old weight (256, 128, 3, 3)\n",
      "==> output_size (256, 4, 4)\n",
      "==> T (256, 16, 128, 100)\n",
      "inp_size: [64, 18, 18]\n",
      "old weight (128, 64, 3, 3)\n",
      "==> output_size (128, 8, 8)\n",
      "==> T (128, 64, 64, 324)\n",
      "inp_size: [3, 34, 34]\n",
      "old weight (64, 3, 3, 3)\n",
      "==> output_size (64, 16, 16)\n",
      "==> T (64, 256, 3, 1156)\n"
     ]
    }
   ],
   "source": [
    "# conv\n",
    "# inv_net.f25.init_layer(net.features[25])\n",
    "# inv_net.f22.init_layer(net.features[22])\n",
    "# inv_net.f18.init_layer(net.features[18])\n",
    "inv_net.f40.init_layer(net.block4[0])\n",
    "inv_net.f30.init_layer(net.block3[0])\n",
    "inv_net.f20.init_layer(net.block2[0])\n",
    "inv_net.f10.init_layer(net.block1[0])\n",
    "inv_net.f00.init_layer(net.block0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_bn(inv_bn, old_bn):\n",
    "#     inv_bn.bias.data = old_bn.running_mean.cpu()\n",
    "#     inv_bn.weight.data = torch.sqrt(old_bn.running_var.cpu()+1e-8)\n",
    "\n",
    "def update_bn(inv_bn, old_bn):\n",
    "    inv_bn.bias.data = old_bn.running_mean.cpu().data\n",
    "    inv_bn.weight.data = torch.sqrt(old_bn.running_var.cpu()+1e-8).data\n",
    "    inv_bn.running_mean.data = old_bn.bias.cpu().data\n",
    "    inv_bn.running_var.data  = (old_bn.weight.cpu()**2).data\n",
    "\n",
    "# bn\n",
    "update_bn(inv_net.f01, net.block0[1])\n",
    "update_bn(inv_net.f11, net.block1[1])\n",
    "update_bn(inv_net.f21, net.block2[1])\n",
    "update_bn(inv_net.f31, net.block3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "realmean = torch.tensor([-0.3295, -0.3372, -0.3112]).view(1,3,1,1)\n",
    "realstd  = torch.tensor([1.3969, 1.3923, 1.4135]).view(1,3,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hehe torch.Size([256, 512])\n",
      "tensor([[[[-0.3928]],\n",
      "\n",
      "         [[-0.3068]],\n",
      "\n",
      "         [[-0.2673]]]])\n",
      "tensor([[[[2.5934]],\n",
      "\n",
      "         [[2.9004]],\n",
      "\n",
      "         [[2.6616]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f64f73b9470>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from di import denormalize\n",
    "img_idx = 1\n",
    "# for class 0\n",
    "saved = torch.load('/project/kung/xin/cifar_vgg11_saved_model/c0.pth')\n",
    "\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    inv_net.train()\n",
    "    out = net(saved['features.0'][0:256], False)\n",
    "    inverted_img = inv_net(out)\n",
    "    # print(inverted_img.size())\n",
    "\n",
    "mean = inverted_img.mean([0,2,3], keepdim=True)\n",
    "std  = inverted_img.std([0,2,3], unbiased=False, keepdim=True)\n",
    "print(mean)\n",
    "print(std)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(denormalize(saved['features.0'][img_idx]).view(3,-1).T.view(32,32,3))\n",
    "plt.figure()\n",
    "plt.imshow(denormalize(inverted_img[img_idx]).reshape(3,-1).T.view(32,32,3).detach())\n",
    "plt.figure()\n",
    "plt.imshow(denormalize(((inverted_img[img_idx]-mean)/std)*realstd +realmean).view(3,-1).T.view(32,32,3).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-788ee363e0a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert 1==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_inp_l = []\n",
    "# for i in range(10):\n",
    "#     all_inp_l.append(torch.load('/project/kung/xin/cifar_vgg11_saved_model/c%d.pth'%i)['features.0'])\n",
    "# all_inp = torch.cat(all_inp_l)\n",
    "\n",
    "# stat_bsz = 128\n",
    "# realinp_dataset = torch.utils.data.TensorDataset(all_inp)\n",
    "# realinp_loader = torch.utils.data.DataLoader(realinp_dataset, batch_size=stat_bsz, shuffle=True)\n",
    "\n",
    "# mean_list = []\n",
    "# std_list = []\n",
    "# for data in realinp_loader:\n",
    "#     mean = data[0].mean([0,2,3])\n",
    "#     std  = data[0].std([0,2,3], unbiased=False)\n",
    "#     mean_list.append(mean)\n",
    "#     std_list.append(std)\n",
    "    \n",
    "# print(mean_list[0].size())\n",
    "\n",
    "# all_mean = torch.stack(mean_list).mean([0])\n",
    "# all_std  = torch.stack(std_list).mean([0])\n",
    "\n",
    "# print(all_mean)\n",
    "# print(all_std)\n",
    "\n",
    "# oPlot = FlowLayout() # create an empty FlowLayout\n",
    "# for i in range(3): \n",
    "#     fig, ax = plt.subplots(1, 1, figsize=(5.9,5.9))\n",
    "#     cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "#     sns.histplot([t[i].item() for t in mean_list], stat='probability', kde=True)\n",
    "#     fig.tight_layout()\n",
    "#     # fig.suptitle('Output of Conv2d', fontsize=16)\n",
    "#     oPlot.add_plot(ax) \n",
    "#     plt.close() \n",
    "# oPlot.PassHtmlToCell()    \n",
    "\n",
    "\n",
    "# oPlot = FlowLayout() # create an empty FlowLayout\n",
    "# for i in range(3): \n",
    "#     fig, ax = plt.subplots(1, 1, figsize=(5.9,5.9))\n",
    "#     cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "#     sns.histplot([t[i].item() for t in std_list], stat='probability', kde=True)\n",
    "#     fig.tight_layout()\n",
    "#     # fig.suptitle('Output of Conv2d', fontsize=16)\n",
    "#     oPlot.add_plot(ax) \n",
    "#     plt.close() \n",
    "# oPlot.PassHtmlToCell()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_to(inp, mean, std):\n",
    "    inp_mean = inp.mean([0,2,3], keepdim=True)\n",
    "    inp_std  = inp.std([0,2,3], unbiased=False, keepdim=True)\n",
    "    return ((inp-inp_mean)/inp_std)*std + mean\n",
    "\n",
    "\n",
    "all_f0out_l = []\n",
    "all_target_l = []\n",
    "all_inp_l = []\n",
    "for i in range(10):\n",
    "    all_f0out_l.append(torch.load('/project/kung/xin/cifar_vgg11_saved_model/c%d.pth'%i)['features.4'])\n",
    "    all_inp_l.append(torch.load('/project/kung/xin/cifar_vgg11_saved_model/c%d.pth'%i)['features.0'])\n",
    "    all_target_l.append(torch.load('/project/kung/xin/cifar_vgg11_saved_model/c%d.pth'%i)['target'])\n",
    "all_f0out = torch.cat(all_f0out_l)\n",
    "all_inp   = torch.cat(all_inp_l)\n",
    "all_target = torch.cat(all_target_l)\n",
    "\n",
    "stat_bsz = 128\n",
    "realinp_dataset = torch.utils.data.TensorDataset(all_f0out, all_inp, all_target)\n",
    "realinp_loader = torch.utils.data.DataLoader(realinp_dataset, batch_size=stat_bsz, shuffle=True)\n",
    "\n",
    "realmean = torch.tensor([-0.3295, -0.3372, -0.3112]).view(1,3,1,1)\n",
    "realstd  = torch.tensor([1.3969, 1.3923, 1.4135]).view(1,3,1,1)\n",
    "\n",
    "\n",
    "net.eval()\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "inv_net.train()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (f0out, inp, targets) in enumerate(realinp_loader):\n",
    "        out, (idx03, idx07, idx14, idx21, idx28) = net(inp, True)\n",
    "        inverted_img = inv_net(f0out, idx03, None, None, None, None)\n",
    "\n",
    "\n",
    "        # input back\n",
    "        outputs = net(normalize_to(inverted_img, realmean, realstd))\n",
    "        # outputs = net(inverted_img)\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        acc = 100.*correct/total\n",
    "        print('Acc: %.4f'%acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "oPlot = FlowLayout() # create an empty FlowLayout\n",
    "\n",
    "# Some fairly regular plotting from Matplotlib\n",
    "gX = np.linspace(-5,5,100) # just used in the plot example\n",
    "for i in range(3): # plot 10 charts\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5.9,5.9)) # same size plots\n",
    "                           # figsize=(3+i/3,2+i/4)) # different size plots\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    sns.heatmap(denormalize(saved['features.0'][img_idx])[i], ax=ax, cmap=cmap, square=True, cbar_kws={\"shrink\": .8}, center=0.)\n",
    "    fig.tight_layout()\n",
    "    # fig.suptitle('Output of Conv2d', fontsize=16)\n",
    "    oPlot.add_plot(ax) # pass it to the FlowLayout to save as an image\n",
    "    plt.close() # this gets rid of the plot so it doesn't appear in the cell\n",
    "\n",
    "\n",
    "oPlot.PassHtmlToCell()\n",
    "\n",
    "\n",
    "\n",
    "oPlot = FlowLayout() # create an empty FlowLayout\n",
    "\n",
    "# Some fairly regular plotting from Matplotlib\n",
    "gX = np.linspace(-5,5,100) # just used in the plot example\n",
    "for i in range(3): # plot 10 charts\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5.9,5.9)) # same size plots\n",
    "                           # figsize=(3+i/3,2+i/4)) # different size plots\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    sns.heatmap(denormalize(((inverted_img[img_idx]-mean)/std)*realstd +realmean)[0][i], ax=ax, cmap=cmap, square=True, cbar_kws={\"shrink\": .8}, center=0.)\n",
    "    fig.tight_layout()\n",
    "    # fig.suptitle('Output of Conv2d', fontsize=16)\n",
    "    oPlot.add_plot(ax) # pass it to the FlowLayout to save as an image\n",
    "    plt.close() # this gets rid of the plot so it doesn't appear in the cell\n",
    "\n",
    "\n",
    "oPlot.PassHtmlToCell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_bn(inv_bn, old_bn):\n",
    "#     inv_bn.bias.data = old_bn.running_mean.cpu()\n",
    "#     inv_bn.weight.data = torch.sqrt(old_bn.running_var.cpu()+1e-8)\n",
    "\n",
    "# def update_bn(inv_bn, old_bn):\n",
    "#     inv_bn.bias.data = old_bn.running_mean.cpu()\n",
    "#     inv_bn.weight.data = torch.sqrt(old_bn.running_var.cpu()+1e-8)\n",
    "#     inv_bn.running_mean.data = old_bn.bias.cpu().data\n",
    "#     inv_bn.running_var.data  = (old_bn.weight.cpu()**2).data\n",
    "\n",
    "# # bn\n",
    "# update_bn(inv_net.f01, net.features[1])\n",
    "# update_bn(inv_net.f05, net.features[5])\n",
    "# update_bn(inv_net.f09, net.features[9])\n",
    "# update_bn(inv_net.f12, net.features[12])\n",
    "# update_bn(inv_net.f16, net.features[16])\n",
    "# update_bn(inv_net.f19, net.features[19])\n",
    "# update_bn(inv_net.f23, net.features[23])\n",
    "# update_bn(inv_net.f26, net.features[26])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc\n",
    "# inv_net.classifier.weight.data = net.classifier.weight.data.T.contiguous().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FeatureHook():\n",
    "#     def __init__(self, module):\n",
    "#         self.hook = module.register_forward_hook(self.hook_fn)\n",
    "\n",
    "#     def hook_fn(self, module, input, output):\n",
    "#         self.r_feature = input[0]\n",
    "\n",
    "#     def close(self):\n",
    "#         self.hook.remove()\n",
    "        \n",
    "        \n",
    "# hook_dict = {}\n",
    "# for n, m in net.named_modules():\n",
    "#     if n!='' and n!='features':\n",
    "#         hook_dict[n] = FeatureHook(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.eval()\n",
    "# # inv_net.train()\n",
    "# inv_net.eval()\n",
    "\n",
    "\n",
    "# device='cpu'\n",
    "\n",
    "# root = '/project/kung/xin/cifar_vgg11_saved_model/gamma_beta_inverted_images/'\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch_idx, (data, target) in enumerate(trainloader):\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "#         output, out_idx = net(data, True)\n",
    "\n",
    "#         # real_features = {n:h.r_feature for n,h in hook_dict.items()}\n",
    "\n",
    "#         inv_input = inv_net(output, out_idx[0], out_idx[1], out_idx[2], out_idx[3], out_idx[4])\n",
    "\n",
    "#         # fake_output, fake_out_idx = net(inv_input, True)\n",
    "\n",
    "#         # fake_features = {n:h.r_feature for n,h in hook_dict.items()}\n",
    "\n",
    "#         # data_display = torch.cat((data,  inv_input), 0)\n",
    "#         # print(\"displaying original input-inverted input pairs\")\n",
    "#         # vutils.save_image(data_display,'vgg_bn_gammabeta.png', normalize=True, scale_each=True, nrow=int(8))\n",
    "\n",
    "#         torch.save({'real_inp':data, 'real_label':target, 'fake_inp':inv_input}, root+'b-%s.pth'%batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_bn_dict = {}\n",
    "\n",
    "# for n, m in net.named_modules():\n",
    "#     if isinstance(m, nn.BatchNorm2d):\n",
    "#         all_bn_dict[n] = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_bn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob, os\n",
    "# import torch\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# from di import denormalize\n",
    "# import numpy as np\n",
    "\n",
    "# from IPython.display import HTML\n",
    "# import io\n",
    "# import base64\n",
    "\n",
    "\n",
    "# class FlowLayout(object):\n",
    "#     ''' A class / object to display plots in a horizontal / flow layout below a cell '''\n",
    "#     def __init__(self):\n",
    "#         # string buffer for the HTML: initially some CSS; images to be appended\n",
    "#         self.sHtml =  \"\"\"\n",
    "#         <style>\n",
    "#         .floating-box {\n",
    "#         display: inline-block;\n",
    "#         margin: 1px;\n",
    "#         border: 1px solid #888888;  \n",
    "#         }\n",
    "#         </style>\n",
    "#         \"\"\"\n",
    "\n",
    "#     def add_plot(self, oAxes):\n",
    "#         ''' Saves a PNG representation of a Matplotlib Axes object '''\n",
    "#         Bio=io.BytesIO() # bytes buffer for the plot\n",
    "#         fig = oAxes.get_figure()\n",
    "#         fig.canvas.print_png(Bio) # make a png of the plot in the buffer\n",
    "\n",
    "#         # encode the bytes as string using base 64 \n",
    "#         sB64Img = base64.b64encode(Bio.getvalue()).decode()\n",
    "#         self.sHtml+= (\n",
    "#             '<div class=\"floating-box\">'+ \n",
    "#             '<img src=\"data:image/png;base64,{}\\n\">'.format(sB64Img)+\n",
    "#             '</div>')\n",
    "\n",
    "#     def PassHtmlToCell(self):\n",
    "#         ''' Final step - display the accumulated HTML '''\n",
    "#         display(HTML(self.sHtml))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# oPlot = FlowLayout() # create an empty FlowLayout\n",
    "\n",
    "# for layer_name in all_bn_dict.keys(): # plot 10 charts\n",
    "#     running_std = torch.sqrt(all_bn_dict[layer_name].running_var)\n",
    "#     running_mean = all_bn_dict[layer_name].running_mean\n",
    "\n",
    "#     real_std = real_features[layer_name].std([0, 2, 3], unbiased=False)\n",
    "#     real_mean = real_features[layer_name].mean([0, 2, 3])\n",
    "\n",
    "#     fake_std = fake_features[layer_name].std([0, 2, 3], unbiased=False)\n",
    "#     fake_mean = fake_features[layer_name].mean([0, 2, 3])\n",
    "    \n",
    "#     fig, ax = plt.subplots(1, 1, figsize=(8,5.9))\n",
    "\n",
    "#     ax.plot(running_mean.detach(), label='running mean', linewidth=3, alpha=0.5)\n",
    "#     ax.plot(real_mean.detach(),    label='mean on real data', linewidth=3, alpha=0.5)\n",
    "#     ax.plot(fake_mean.detach(),    label='mean on inverted data', linewidth=3, alpha=0.5)\n",
    "#     ax.set_title('%s Certain Channel'%layer_name, fontsize=23)\n",
    "#     ax.set_xlabel('channel index')\n",
    "#     ax.legend(fontsize=20)\n",
    "    \n",
    "#     fig.tight_layout()\n",
    "#     oPlot.add_plot(ax) # pass it to the FlowLayout to save as an image\n",
    "#     plt.close() # this gets rid of the plot so it doesn't appear in the cell\n",
    "\n",
    "\n",
    "# oPlot.PassHtmlToCell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FeatureHook():\n",
    "#     def __init__(self, module):\n",
    "#         self.hook = module.register_forward_hook(self.hook_fn)\n",
    "\n",
    "#     def hook_fn(self, module, input, output):\n",
    "#         self.r_feature = input[0]\n",
    "\n",
    "#     def close(self):\n",
    "#         self.hook.remove()\n",
    "        \n",
    "        \n",
    "# hook_dict = {}\n",
    "# for n, m in net.named_modules():\n",
    "#     if n!='' and n!='features':\n",
    "#         hook_dict[n] = FeatureHook(m)\n",
    "        \n",
    "# root_dir = '/project/kung/xin/cifar_vgg11_saved_model/'\n",
    "        \n",
    "# for class_idx in range(10):\n",
    "#     t = np.array(trainset.targets)\n",
    "#     indices = np.argwhere(t==class_idx).flatten().tolist()\n",
    "#     sub_set = torch.utils.data.Subset(trainset, indices)\n",
    "#     sub_loader = torch.utils.data.DataLoader(\n",
    "#         sub_set, batch_size=len(sub_set), shuffle=False, num_workers=8)\n",
    "#     for image, label in sub_loader:\n",
    "#         output = net(image)\n",
    "        \n",
    "#     save_dict = {n:h.r_feature for n, h in hook_dict.items()}\n",
    "#     save_dict['output'] = output\n",
    "    \n",
    "#     torch.save(save_dict, root_dir+'c'+str(class_idx)+'.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
