{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1,2,3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#GPU:  4\n",
      "PyTorch Version: 1.6.0+cu92\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from utils import progress_bar\n",
    "\n",
    "\n",
    "from pprint import pprint as pp\n",
    "from scipy import linalg\n",
    "\n",
    "print('#GPU: ', torch.cuda.device_count())\n",
    "print('PyTorch Version:', torch.__version__)\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toeplitz_1_ch(kernel, input_size):\n",
    "    # shapes\n",
    "    k_h, k_w = kernel.shape\n",
    "    i_h, i_w = input_size\n",
    "    o_h, o_w = i_h-k_h+1, i_w-k_w+1\n",
    "\n",
    "    # construct 1d conv toeplitz matrices for each row of the kernel\n",
    "    toeplitz = []\n",
    "    for r in range(k_h):\n",
    "        toeplitz.append(linalg.toeplitz(c=(kernel[r,0], *np.zeros(i_w-k_w)), r=(*kernel[r], *np.zeros(i_w-k_w))) ) \n",
    "\n",
    "    # construct toeplitz matrix of toeplitz matrices (just for padding=0)\n",
    "    h_blocks, w_blocks = o_h, i_h\n",
    "    h_block, w_block = toeplitz[0].shape\n",
    "\n",
    "    W_conv = np.zeros((h_blocks, h_block, w_blocks, w_block))\n",
    "\n",
    "    for i, B in enumerate(toeplitz):\n",
    "        for j in range(o_h):\n",
    "            W_conv[j, :, i+j, :] = B\n",
    "\n",
    "    W_conv.shape = (h_blocks*h_block, w_blocks*w_block)\n",
    "\n",
    "    return W_conv\n",
    "\n",
    "def toeplitz_mult_ch(kernel, input_size):\n",
    "    \"\"\"Compute toeplitz matrix for 2d conv with multiple in and out channels.\n",
    "    Args:\n",
    "        kernel: shape=(n_out, n_in, H_k, W_k)\n",
    "        input_size: (n_in, H_i, W_i)\"\"\"\n",
    "\n",
    "    kernel_size = kernel.shape\n",
    "    output_size = (kernel_size[0], input_size[1] - (kernel_size[2]-1), input_size[2] - (kernel_size[3]-1))\n",
    "    print('==> output_size', output_size)\n",
    "    T = np.zeros((output_size[0], int(np.prod(output_size[1:])), input_size[0], int(np.prod(input_size[1:]))))\n",
    "    print('==> T', T.shape)\n",
    "\n",
    "    for i,ks in enumerate(kernel):  # loop over output channel\n",
    "        for j,k in enumerate(ks):  # loop over input channel\n",
    "            T_k = toeplitz_1_ch(k, input_size[1:])\n",
    "            T[i, :, j, :] = T_k\n",
    "\n",
    "    T.shape = (np.prod(output_size), np.prod(input_size))\n",
    "\n",
    "    return T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InvConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InvConv, self).__init__()\n",
    "        self.if_init = False\n",
    "\n",
    "    def init_layer(self, old_layer):\n",
    "        self.old_weight = old_layer.weight.detach().cpu().clone().numpy()\n",
    "        self.inp_padding = old_layer.padding\n",
    "        \n",
    "        self.inp_size = list(old_layer.inp_size)[1:] # inp_size was obtained by hook\n",
    "        \n",
    "        \n",
    "        if old_layer.padding[-1]>0:\n",
    "            self.inp_size[-1] += 2*old_layer.padding[-1]\n",
    "        if old_layer.padding[-2]>0:\n",
    "            self.inp_size[-2] += 2*old_layer.padding[-2]\n",
    "            \n",
    "        print('inp_size:', self.inp_size)\n",
    "        print('old weight', self.old_weight.shape)\n",
    "        self.old_weight_matrix = toeplitz_mult_ch(\n",
    "            self.old_weight, self.inp_size)\n",
    "        self.old_weight_matrix = torch.tensor(self.old_weight_matrix).float()\n",
    "        \n",
    "        \n",
    "        self.if_init = True\n",
    "        \n",
    "    def forward(self, y, if_inv=True):\n",
    "        '''\n",
    "        Problem: \n",
    "            1. the converted weight matrix is super large. This matrix may consum ~150G memory. Please do NOT use cuda. \n",
    "        '''\n",
    "        \n",
    "        assert self.if_init\n",
    "\n",
    "        # remember to transpose the old_weight_matrix\n",
    "        self.old_weight_matrix = self.old_weight_matrix.to(y.device)\n",
    "            \n",
    "        if if_inv:\n",
    "            '''\n",
    "            inversion of convolution\n",
    "            '''\n",
    "            y = y.contiguous()\n",
    "            out = torch.matmul(self.old_weight_matrix.t()[None,:,:], y.view(y.size(0), -1)[:,:,None])\n",
    "\n",
    "            # reshape the output\n",
    "            out = out.view([out.size(0)] + self.inp_size)\n",
    "\n",
    "            # un-padding\n",
    "            if self.inp_padding[0] == 0 and self.inp_padding[1]==0:\n",
    "                out_unpadding = out\n",
    "            else:\n",
    "                out_unpadding = out[:,:,self.inp_padding[0]:-self.inp_padding[0],self.inp_padding[1]:-self.inp_padding[1]]\n",
    "\n",
    "            return out_unpadding\n",
    "        else:\n",
    "            '''\n",
    "            standard convolution. However, we implement the convolution with pure matrix(weights)-vector(input) multiplication.\n",
    "            \n",
    "            '''\n",
    "            x = y\n",
    "            x_pad = F.pad(x, pad=[self.inp_padding[0],self.inp_padding[0],self.inp_padding[1],self.inp_padding[1]])\n",
    "            print('### x_pad', x_pad.view(x_pad.size(0), -1)[:,:,None].size())\n",
    "            out = torch.matmul(self.old_weight_matrix[None,:,:], x_pad.view(x_pad.size(0), -1)[:,:,None])\n",
    "            # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv#torch.nn.Conv2d\n",
    "            out_size = int( (self.inp_size[-1] - 1 * (self.old_weight.shape[-1] -1) - 1)  / 1. + 1)\n",
    "            out = out.view([out.size(0), self.old_weight.shape[0]]+[out_size]*2)\n",
    "            return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x, out_idx=False):\n",
    "        # out = self.features(x)\n",
    "        out = self.features[0](x)\n",
    "        out = self.features[1](out)\n",
    "        out = self.features[2](out)\n",
    "        out, idx03 = self.features[3](out)\n",
    "        out = self.features[4](out)\n",
    "        out = self.features[5](out)\n",
    "        out = self.features[6](out)\n",
    "        out, idx07 = self.features[7](out)\n",
    "        out = self.features[8](out)\n",
    "        out = self.features[9](out)\n",
    "        out = self.features[10](out)\n",
    "        out = self.features[11](out)\n",
    "        out = self.features[12](out)\n",
    "        out = self.features[13](out)\n",
    "        out, idx14 = self.features[14](out)\n",
    "        out = self.features[15](out)\n",
    "        out = self.features[16](out)\n",
    "        out = self.features[17](out)\n",
    "        out = self.features[18](out)\n",
    "        out = self.features[19](out)\n",
    "        out = self.features[20](out)\n",
    "        out, idx21 = self.features[21](out)\n",
    "        out = self.features[22](out)\n",
    "        out = self.features[23](out)\n",
    "        out = self.features[24](out)\n",
    "        out = self.features[25](out)\n",
    "        out = self.features[26](out)\n",
    "        out = self.features[27](out)\n",
    "        out, idx28 = self.features[28](out)\n",
    "        out = self.features[29](out)\n",
    "        \n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        \n",
    "        if out_idx:\n",
    "            return out, (idx03, idx07, idx14, idx21, idx28)\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=False)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Preparing data..\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='/dev/shm', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=64, shuffle=True, num_workers=8)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='/dev/shm', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=64, shuffle=False, num_workers=8)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================================================>]  Step: 25ms | Tot: 9s4ms | Loss: 0.466 | Acc: 89.690% (8969/10000) 157/157  .....]  Step: 78ms | Tot: 5s282ms | Loss: 0.465 | Acc: 89.802% (5345/5952) 93/157 \n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "lr = 0.01\n",
    "epochs = 100\n",
    "best_acc = 0\n",
    "\n",
    "net = VGG('VGG11').to(device)\n",
    "# if device == 'cuda':\n",
    "#     net = torch.nn.DataParallel(net)\n",
    "#     cudnn.benchmark = True\n",
    "\n",
    "# if args.resume:\n",
    "#     # Load checkpoint.\n",
    "#     print('==> Resuming from checkpoint..')\n",
    "#     assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "#     checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "#     net.load_state_dict(checkpoint['net'])\n",
    "#     best_acc = checkpoint['acc']\n",
    "#     start_epoch = checkpoint['epoch']\n",
    "start_epoch = 0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, eta_min=0, last_epoch=-1)\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        # state = {\n",
    "        #     'net': net.module.state_dict(),\n",
    "        #     'acc': acc,\n",
    "        #     'epoch': epoch,\n",
    "        # }\n",
    "        # torch.save(state, './cifar_vgg_11.pth')\n",
    "        best_acc = acc\n",
    "\n",
    "\n",
    "# for epoch in range(start_epoch, start_epoch+epochs):\n",
    "#     train(epoch)\n",
    "#     test(epoch)\n",
    "#     scheduler.step()\n",
    "\n",
    "checkpoint = torch.load('./cifar_vgg_11.pth')\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "\n",
    "\n",
    "        \n",
    "def InpSizeHook(module, input, output):\n",
    "    module.inp_size = input[0].size()\n",
    "    \n",
    "for n,m in net.named_modules():\n",
    "    if n!='' and n!='features':\n",
    "        m.register_forward_hook(InpSizeHook)\n",
    "    \n",
    "test(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0 || Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) || torch.Size([16, 3, 32, 32])\n",
      "features.1 || BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) || torch.Size([16, 64, 32, 32])\n",
      "features.2 || ReLU() || torch.Size([16, 64, 32, 32])\n",
      "features.3 || MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) || torch.Size([16, 64, 32, 32])\n",
      "features.4 || Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) || torch.Size([16, 64, 16, 16])\n",
      "features.5 || BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) || torch.Size([16, 128, 16, 16])\n",
      "features.6 || ReLU() || torch.Size([16, 128, 16, 16])\n",
      "features.7 || MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) || torch.Size([16, 128, 16, 16])\n",
      "features.8 || Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) || torch.Size([16, 128, 8, 8])\n",
      "features.9 || BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) || torch.Size([16, 256, 8, 8])\n",
      "features.10 || ReLU() || torch.Size([16, 256, 8, 8])\n",
      "features.11 || Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) || torch.Size([16, 256, 8, 8])\n",
      "features.12 || BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) || torch.Size([16, 256, 8, 8])\n",
      "features.13 || ReLU() || torch.Size([16, 256, 8, 8])\n",
      "features.14 || MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) || torch.Size([16, 256, 8, 8])\n",
      "features.15 || Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) || torch.Size([16, 256, 4, 4])\n",
      "features.16 || BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) || torch.Size([16, 512, 4, 4])\n",
      "features.17 || ReLU() || torch.Size([16, 512, 4, 4])\n",
      "features.18 || Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) || torch.Size([16, 512, 4, 4])\n",
      "features.19 || BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) || torch.Size([16, 512, 4, 4])\n",
      "features.20 || ReLU() || torch.Size([16, 512, 4, 4])\n",
      "features.21 || MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) || torch.Size([16, 512, 4, 4])\n",
      "features.22 || Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) || torch.Size([16, 512, 2, 2])\n",
      "features.23 || BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) || torch.Size([16, 512, 2, 2])\n",
      "features.24 || ReLU() || torch.Size([16, 512, 2, 2])\n",
      "features.25 || Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) || torch.Size([16, 512, 2, 2])\n",
      "features.26 || BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) || torch.Size([16, 512, 2, 2])\n",
      "features.27 || ReLU() || torch.Size([16, 512, 2, 2])\n",
      "features.28 || MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) || torch.Size([16, 512, 2, 2])\n",
      "features.29 || AvgPool2d(kernel_size=1, stride=1, padding=0) || torch.Size([16, 512, 1, 1])\n",
      "classifier || Linear(in_features=512, out_features=10, bias=True) || torch.Size([16, 512])\n"
     ]
    }
   ],
   "source": [
    "for n,m in net.named_modules():\n",
    "    if n!='' and n!='features':\n",
    "        print(n,'||',m, '||',m.inp_size)\n",
    "        # print()\n",
    "        # if '9' in n:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvVGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InvVGG, self).__init__()\n",
    "        self.classifier = nn.Linear(10, 512)\n",
    "        self.unpool = nn.MaxUnpool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.f26 = nn.BatchNorm2d(512)\n",
    "        self.f25 = InvConv()\n",
    "        self.f23 = nn.BatchNorm2d(512)\n",
    "        self.f22 = InvConv()\n",
    "        self.f19 = nn.BatchNorm2d(512)\n",
    "        self.f18 = InvConv()\n",
    "        self.f16 = nn.BatchNorm2d(512)\n",
    "        self.f15 = InvConv()\n",
    "        self.f12 = nn.BatchNorm2d(256)\n",
    "        self.f11 = InvConv()\n",
    "        self.f09 = nn.BatchNorm2d(256)\n",
    "        self.f08 = InvConv()\n",
    "        self.f05 = nn.BatchNorm2d(128)\n",
    "        self.f04 = InvConv()\n",
    "        self.f01 = nn.BatchNorm2d(64)\n",
    "        self.f00 = InvConv()\n",
    "        \n",
    "    def forward(self, x, idx03, idx07, idx14, idx21, idx28):\n",
    "        out = self.classifier(x)\n",
    "        out = out.view(out.size(0), 512, 1, 1)\n",
    "        \n",
    "        out = self.unpool(out, idx28)  # f 28, pool\n",
    "        # print('out 28:', out.size())\n",
    "        out = self.f26(out)            # f 26, bn\n",
    "        # print('out 26:', out.size())\n",
    "        out = self.f25(out)            # f 25, conv\n",
    "        out = self.f23(out)              # f 23, bn\n",
    "        out = self.f22(out)              # f 22, conv\n",
    "        out = self.unpool(out, idx21)  # f 21, pool\n",
    "        out = self.f19(out)            # f 19, bn\n",
    "        out = self.f18(out)            # f 18, conv\n",
    "        out = self.f16(out)            # f 16, bn\n",
    "        out = self.f15(out)            # f 15, conv\n",
    "        out = self.unpool(out, idx14)  # f 14, pool\n",
    "        out = self.f12(out)            # f 12, bn\n",
    "        out = self.f11(out)            # f 11, conv\n",
    "        out = self.f09(out)            # f 09, bn\n",
    "        out = self.f08(out)            # f 08, conv\n",
    "        out = self.unpool(out, idx07)  # f 07, pool\n",
    "        out = self.f05(out)            # f 05, bn\n",
    "        out = self.f04(out)            # f 04, conv\n",
    "        out = self.unpool(out, idx03)  # f 03, pool\n",
    "        out = self.f01(out)            # f 01, bn\n",
    "        out = self.f00(out)            # f 00, conv\n",
    "        \n",
    "        return out\n",
    "    \n",
    "inv_net = InvVGG()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "dx = nn.BatchNorm2d(1)\n",
    "\n",
    "print(dx.weight)\n",
    "print(dx.bias)\n",
    "print(dx.running_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp_size: [512, 4, 4]\n",
      "old weight (512, 512, 3, 3)\n",
      "==> output_size (512, 2, 2)\n",
      "==> T (512, 4, 512, 16)\n",
      "inp_size: [512, 4, 4]\n",
      "old weight (512, 512, 3, 3)\n",
      "==> output_size (512, 2, 2)\n",
      "==> T (512, 4, 512, 16)\n",
      "inp_size: [512, 6, 6]\n",
      "old weight (512, 512, 3, 3)\n",
      "==> output_size (512, 4, 4)\n",
      "==> T (512, 16, 512, 36)\n",
      "inp_size: [256, 6, 6]\n",
      "old weight (512, 256, 3, 3)\n",
      "==> output_size (512, 4, 4)\n",
      "==> T (512, 16, 256, 36)\n",
      "inp_size: [256, 10, 10]\n",
      "old weight (256, 256, 3, 3)\n",
      "==> output_size (256, 8, 8)\n",
      "==> T (256, 64, 256, 100)\n",
      "inp_size: [128, 10, 10]\n",
      "old weight (256, 128, 3, 3)\n",
      "==> output_size (256, 8, 8)\n",
      "==> T (256, 64, 128, 100)\n",
      "inp_size: [64, 18, 18]\n",
      "old weight (128, 64, 3, 3)\n",
      "==> output_size (128, 16, 16)\n",
      "==> T (128, 256, 64, 324)\n",
      "inp_size: [3, 34, 34]\n",
      "old weight (64, 3, 3, 3)\n",
      "==> output_size (64, 32, 32)\n",
      "==> T (64, 1024, 3, 1156)\n"
     ]
    }
   ],
   "source": [
    "# conv\n",
    "inv_net.f25.init_layer(net.features[25])\n",
    "inv_net.f22.init_layer(net.features[22])\n",
    "inv_net.f18.init_layer(net.features[18])\n",
    "inv_net.f15.init_layer(net.features[15])\n",
    "inv_net.f11.init_layer(net.features[11])\n",
    "inv_net.f08.init_layer(net.features[8])\n",
    "inv_net.f04.init_layer(net.features[4])\n",
    "inv_net.f00.init_layer(net.features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_bn(inv_bn, old_bn):\n",
    "#     inv_bn.bias.data = old_bn.running_mean.cpu()\n",
    "#     inv_bn.weight.data = torch.sqrt(old_bn.running_var.cpu()+1e-8)\n",
    "\n",
    "def update_bn(inv_bn, old_bn):\n",
    "    inv_bn.bias.data = old_bn.running_mean.cpu()\n",
    "    inv_bn.weight.data = torch.sqrt(old_bn.running_var.cpu()+1e-8)\n",
    "    inv_bn.running_mean.data = old_bn.bias.cpu().data\n",
    "    inv_bn.running_var.data  = (old_bn.weight.cpu()**2).data\n",
    "\n",
    "# bn\n",
    "update_bn(inv_net.f01, net.features[1])\n",
    "update_bn(inv_net.f05, net.features[5])\n",
    "update_bn(inv_net.f09, net.features[9])\n",
    "update_bn(inv_net.f12, net.features[12])\n",
    "update_bn(inv_net.f16, net.features[16])\n",
    "update_bn(inv_net.f19, net.features[19])\n",
    "update_bn(inv_net.f23, net.features[23])\n",
    "update_bn(inv_net.f26, net.features[26])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc\n",
    "inv_net.classifier.weight.data = net.classifier.weight.data.T.contiguous().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FeatureHook():\n",
    "#     def __init__(self, module):\n",
    "#         self.hook = module.register_forward_hook(self.hook_fn)\n",
    "\n",
    "#     def hook_fn(self, module, input, output):\n",
    "#         self.r_feature = input[0]\n",
    "\n",
    "#     def close(self):\n",
    "#         self.hook.remove()\n",
    "        \n",
    "        \n",
    "# hook_dict = {}\n",
    "# for n, m in net.named_modules():\n",
    "#     if n!='' and n!='features':\n",
    "#         hook_dict[n] = FeatureHook(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "# inv_net.train()\n",
    "inv_net.eval()\n",
    "\n",
    "\n",
    "device='cpu'\n",
    "\n",
    "root = '/project/kung/xin/cifar_vgg11_saved_model/gamma_beta_inverted_images/'\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output, out_idx = net(data, True)\n",
    "\n",
    "        # real_features = {n:h.r_feature for n,h in hook_dict.items()}\n",
    "\n",
    "        inv_input = inv_net(output, out_idx[0], out_idx[1], out_idx[2], out_idx[3], out_idx[4])\n",
    "\n",
    "        # fake_output, fake_out_idx = net(inv_input, True)\n",
    "\n",
    "        # fake_features = {n:h.r_feature for n,h in hook_dict.items()}\n",
    "\n",
    "        # data_display = torch.cat((data,  inv_input), 0)\n",
    "        # print(\"displaying original input-inverted input pairs\")\n",
    "        # vutils.save_image(data_display,'vgg_bn_gammabeta.png', normalize=True, scale_each=True, nrow=int(8))\n",
    "\n",
    "        torch.save({'real_inp':data, 'real_label':target, 'fake_inp':inv_input}, root+'b-%s.pth'%batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_bn_dict = {}\n",
    "\n",
    "# for n, m in net.named_modules():\n",
    "#     if isinstance(m, nn.BatchNorm2d):\n",
    "#         all_bn_dict[n] = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_bn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob, os\n",
    "# import torch\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# from di import denormalize\n",
    "# import numpy as np\n",
    "\n",
    "# from IPython.display import HTML\n",
    "# import io\n",
    "# import base64\n",
    "\n",
    "\n",
    "# class FlowLayout(object):\n",
    "#     ''' A class / object to display plots in a horizontal / flow layout below a cell '''\n",
    "#     def __init__(self):\n",
    "#         # string buffer for the HTML: initially some CSS; images to be appended\n",
    "#         self.sHtml =  \"\"\"\n",
    "#         <style>\n",
    "#         .floating-box {\n",
    "#         display: inline-block;\n",
    "#         margin: 1px;\n",
    "#         border: 1px solid #888888;  \n",
    "#         }\n",
    "#         </style>\n",
    "#         \"\"\"\n",
    "\n",
    "#     def add_plot(self, oAxes):\n",
    "#         ''' Saves a PNG representation of a Matplotlib Axes object '''\n",
    "#         Bio=io.BytesIO() # bytes buffer for the plot\n",
    "#         fig = oAxes.get_figure()\n",
    "#         fig.canvas.print_png(Bio) # make a png of the plot in the buffer\n",
    "\n",
    "#         # encode the bytes as string using base 64 \n",
    "#         sB64Img = base64.b64encode(Bio.getvalue()).decode()\n",
    "#         self.sHtml+= (\n",
    "#             '<div class=\"floating-box\">'+ \n",
    "#             '<img src=\"data:image/png;base64,{}\\n\">'.format(sB64Img)+\n",
    "#             '</div>')\n",
    "\n",
    "#     def PassHtmlToCell(self):\n",
    "#         ''' Final step - display the accumulated HTML '''\n",
    "#         display(HTML(self.sHtml))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# oPlot = FlowLayout() # create an empty FlowLayout\n",
    "\n",
    "# for layer_name in all_bn_dict.keys(): # plot 10 charts\n",
    "#     running_std = torch.sqrt(all_bn_dict[layer_name].running_var)\n",
    "#     running_mean = all_bn_dict[layer_name].running_mean\n",
    "\n",
    "#     real_std = real_features[layer_name].std([0, 2, 3], unbiased=False)\n",
    "#     real_mean = real_features[layer_name].mean([0, 2, 3])\n",
    "\n",
    "#     fake_std = fake_features[layer_name].std([0, 2, 3], unbiased=False)\n",
    "#     fake_mean = fake_features[layer_name].mean([0, 2, 3])\n",
    "    \n",
    "#     fig, ax = plt.subplots(1, 1, figsize=(8,5.9))\n",
    "\n",
    "#     ax.plot(running_mean.detach(), label='running mean', linewidth=3, alpha=0.5)\n",
    "#     ax.plot(real_mean.detach(),    label='mean on real data', linewidth=3, alpha=0.5)\n",
    "#     ax.plot(fake_mean.detach(),    label='mean on inverted data', linewidth=3, alpha=0.5)\n",
    "#     ax.set_title('%s Certain Channel'%layer_name, fontsize=23)\n",
    "#     ax.set_xlabel('channel index')\n",
    "#     ax.legend(fontsize=20)\n",
    "    \n",
    "#     fig.tight_layout()\n",
    "#     oPlot.add_plot(ax) # pass it to the FlowLayout to save as an image\n",
    "#     plt.close() # this gets rid of the plot so it doesn't appear in the cell\n",
    "\n",
    "\n",
    "# oPlot.PassHtmlToCell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FeatureHook():\n",
    "#     def __init__(self, module):\n",
    "#         self.hook = module.register_forward_hook(self.hook_fn)\n",
    "\n",
    "#     def hook_fn(self, module, input, output):\n",
    "#         self.r_feature = input[0]\n",
    "\n",
    "#     def close(self):\n",
    "#         self.hook.remove()\n",
    "        \n",
    "        \n",
    "# hook_dict = {}\n",
    "# for n, m in net.named_modules():\n",
    "#     if n!='' and n!='features':\n",
    "#         hook_dict[n] = FeatureHook(m)\n",
    "        \n",
    "# root_dir = '/project/kung/xin/cifar_vgg11_saved_model/'\n",
    "        \n",
    "# for class_idx in range(10):\n",
    "#     t = np.array(trainset.targets)\n",
    "#     indices = np.argwhere(t==class_idx).flatten().tolist()\n",
    "#     sub_set = torch.utils.data.Subset(trainset, indices)\n",
    "#     sub_loader = torch.utils.data.DataLoader(\n",
    "#         sub_set, batch_size=len(sub_set), shuffle=False, num_workers=8)\n",
    "#     for image, label in sub_loader:\n",
    "#         output = net(image)\n",
    "        \n",
    "#     save_dict = {n:h.r_feature for n, h in hook_dict.items()}\n",
    "#     save_dict['output'] = output\n",
    "    \n",
    "#     torch.save(save_dict, root_dir+'c'+str(class_idx)+'.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
