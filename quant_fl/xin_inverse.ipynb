{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#GPU:  8\n",
      "PyTorch Version: 1.6.0+cu92\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import pdb\n",
    "import torchvision.utils as vutils\n",
    "from pprint import pprint as pp\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "print('#GPU: ', torch.cuda.device_count())\n",
    "print('PyTorch Version:', torch.__version__)\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref Shape: torch.Size([1, 1, 3, 3])\n",
      "---> [1, 4, 4]\n",
      "==> output_size (1, 3, 3)\n",
      "==> T (1, 9, 1, 16)\n",
      "array([[ 1.73,  0.11,  0.  ,  0.  , -0.24, -0.2 ,  0.  ,  0.  ,  0.  ,\n",
      "         0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
      "       [ 0.  ,  1.73,  0.11,  0.  ,  0.  , -0.24, -0.2 ,  0.  ,  0.  ,\n",
      "         0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
      "       [ 0.  ,  0.  ,  1.73,  0.11,  0.  ,  0.  , -0.24, -0.2 ,  0.  ,\n",
      "         0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
      "       [ 0.  ,  0.  ,  0.  ,  0.  ,  1.73,  0.11,  0.  ,  0.  , -0.24,\n",
      "        -0.2 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
      "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.73,  0.11,  0.  ,  0.  ,\n",
      "        -0.24, -0.2 ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
      "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.73,  0.11,  0.  ,\n",
      "         0.  , -0.24, -0.2 ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
      "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.73,\n",
      "         0.11,  0.  ,  0.  , -0.24, -0.2 ,  0.  ,  0.  ],\n",
      "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
      "         1.73,  0.11,  0.  ,  0.  , -0.24, -0.2 ,  0.  ],\n",
      "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
      "         0.  ,  1.73,  0.11,  0.  ,  0.  , -0.24, -0.2 ]])\n",
      "Weight Matrix, T Shape: (9, 16)\n",
      "Out Shape: torch.Size([1, 9, 1])\n",
      "Out_reshape Shape: torch.Size([1, 1, 3, 3])\n",
      "tensor(2.2204e-16, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def toeplitz_1_ch(kernel, input_size):\n",
    "    # shapes\n",
    "    k_h, k_w = kernel.shape\n",
    "    i_h, i_w = input_size\n",
    "    o_h, o_w = i_h-k_h+1, i_w-k_w+1\n",
    "\n",
    "    # construct 1d conv toeplitz matrices for each row of the kernel\n",
    "    toeplitz = []\n",
    "    for r in range(k_h):\n",
    "        toeplitz.append(linalg.toeplitz(c=(kernel[r,0], *np.zeros(i_w-k_w)), r=(*kernel[r], *np.zeros(i_w-k_w))) ) \n",
    "\n",
    "    # construct toeplitz matrix of toeplitz matrices (just for padding=0)\n",
    "    h_blocks, w_blocks = o_h, i_h\n",
    "    h_block, w_block = toeplitz[0].shape\n",
    "\n",
    "    W_conv = np.zeros((h_blocks, h_block, w_blocks, w_block))\n",
    "\n",
    "    for i, B in enumerate(toeplitz):\n",
    "        for j in range(o_h):\n",
    "            W_conv[j, :, i+j, :] = B\n",
    "\n",
    "    W_conv.shape = (h_blocks*h_block, w_blocks*w_block)\n",
    "\n",
    "    return W_conv\n",
    "\n",
    "def toeplitz_mult_ch(kernel, input_size):\n",
    "    \"\"\"Compute toeplitz matrix for 2d conv with multiple in and out channels.\n",
    "    Args:\n",
    "        kernel: shape=(n_out, n_in, H_k, W_k)\n",
    "        input_size: (n_in, H_i, W_i)\"\"\"\n",
    "\n",
    "    kernel_size = kernel.shape\n",
    "    output_size = (kernel_size[0], input_size[1] - (kernel_size[2]-1), input_size[2] - (kernel_size[3]-1))\n",
    "    print('==> output_size', output_size)\n",
    "    T = np.zeros((output_size[0], int(np.prod(output_size[1:])), input_size[0], int(np.prod(input_size[1:]))))\n",
    "    print('==> T', T.shape)\n",
    "\n",
    "    for i,ks in enumerate(kernel):  # loop over output channel\n",
    "        for j,k in enumerate(ks):  # loop over input channel\n",
    "            T_k = toeplitz_1_ch(k, input_size[1:])\n",
    "            T[i, :, j, :] = T_k\n",
    "\n",
    "    T.shape = (np.prod(output_size), np.prod(input_size))\n",
    "\n",
    "    return T\n",
    "\n",
    "k = np.random.randn(1*1*2*2).reshape((1,1,2,2))\n",
    "i = np.random.randn(1,1,4,4)\n",
    "\n",
    "ref = F.conv2d(torch.tensor(i), torch.tensor(k), padding=0)\n",
    "print('Ref Shape:', ref.shape)\n",
    "\n",
    "i_pad = F.pad(torch.tensor(i), pad=[0,0,0,0])\n",
    "\n",
    "print('--->', list(i_pad.size())[1:])\n",
    "T = toeplitz_mult_ch(k, list(i_pad.size())[1:])\n",
    "pp(T)\n",
    "print('Weight Matrix, T Shape:', T.shape)\n",
    "# ### Do conv as matrix (weight, T) * vector (input)\n",
    "# out = T.dot(i_pad.numpy().flatten())\n",
    "# print('Out Shape:', out.shape)\n",
    "\n",
    "\n",
    "# out_reshape = out.reshape(1,4,7,7)\n",
    "# print('Out_reshape Shape:', out_reshape.shape)\n",
    "\n",
    "\n",
    "# print(np.sum((out_reshape - ref)**2))\n",
    "\n",
    "out = torch.matmul(torch.tensor(T)[None,:,:], i_pad.reshape(i_pad.size(0), -1)[:,:,None])\n",
    "print('Out Shape:', out.size())\n",
    "\n",
    "out_reshape = out.reshape(1,1,3,3)\n",
    "print('Out_reshape Shape:', out_reshape.size())\n",
    "\n",
    "print((ref-out_reshape).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp_size: [3, 6, 6]\n",
      "old weight (5, 3, 3, 3)\n",
      "==> output_size (5, 4, 4)\n",
      "==> T (5, 16, 3, 36)\n",
      "nn Ref shape: torch.Size([1, 1, 3, 3])\n",
      "### x_pad torch.Size([2, 108, 1])\n",
      "tensor(3.5763e-07, grad_fn=<MaxBackward1>)\n",
      "tensor(3.5763e-07, grad_fn=<MaxBackward1>)\n",
      "tensor([[[[ 0.6956, -0.1893,  0.5630,  0.2856],\n",
      "          [ 0.1645,  0.5064, -0.2672, -0.2958],\n",
      "          [ 0.1333,  0.6983,  0.2453,  0.0849],\n",
      "          [ 0.7977,  0.0585,  0.8783, -0.2751]],\n",
      "\n",
      "         [[-0.0515, -0.2804, -0.3157,  0.5948],\n",
      "          [-0.5185,  0.1907,  1.2124, -0.3263],\n",
      "          [-0.3546, -0.2959,  0.1206,  0.1996],\n",
      "          [ 0.2462,  0.6708, -0.7042,  0.5976]],\n",
      "\n",
      "         [[ 0.4417,  0.5273,  0.4430,  0.5128],\n",
      "          [ 0.2436, -0.8946, -0.0797,  0.1172],\n",
      "          [ 0.5668,  0.3461,  0.1307,  0.2035],\n",
      "          [ 0.1604,  0.5501,  0.5731,  0.0167]],\n",
      "\n",
      "         [[-0.5004,  0.0868,  0.6181,  0.0332],\n",
      "          [-0.5029, -0.3809,  0.5123,  0.0351],\n",
      "          [-0.4592, -0.1736, -0.5052,  1.1377],\n",
      "          [-0.0333, -0.5458, -0.2187,  0.3438]],\n",
      "\n",
      "         [[-0.1665,  0.8854, -0.3702, -0.5449],\n",
      "          [-0.1738, -0.3198,  0.2138,  0.3196],\n",
      "          [ 0.3985,  0.4793,  0.0230,  0.1712],\n",
      "          [-0.4910,  0.3222,  0.1348, -0.0708]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0531,  0.1461,  0.6232,  0.0718],\n",
      "          [-0.6251, -0.0193,  0.1624,  0.3255],\n",
      "          [ 0.4257,  0.4789,  0.7777, -0.6988],\n",
      "          [-0.4430, -0.6492,  0.1970, -0.5754]],\n",
      "\n",
      "         [[ 0.0762, -0.3786, -0.0747, -0.2344],\n",
      "          [ 0.6301, -0.6136,  0.0673, -0.6432],\n",
      "          [-0.1791, -0.0781, -0.0333,  0.9803],\n",
      "          [-0.0750,  0.6129,  0.3926,  0.2010]],\n",
      "\n",
      "         [[ 0.3694,  0.4786,  0.2073,  0.0356],\n",
      "          [-0.2059,  0.5221, -0.5048,  0.1516],\n",
      "          [ 0.6729,  0.7409, -0.7168,  0.4773],\n",
      "          [-0.7519,  0.6723, -0.5056,  0.5694]],\n",
      "\n",
      "         [[-0.0485, -0.3119, -0.0690, -0.1777],\n",
      "          [ 0.4202, -0.6374,  0.1486,  0.7112],\n",
      "          [ 0.4979, -0.3066, -0.0874,  0.0450],\n",
      "          [-0.2004,  0.9521, -0.6918,  0.0363]],\n",
      "\n",
      "         [[ 0.3600,  0.3155, -0.2562, -0.1546],\n",
      "          [-0.0132, -0.0780,  0.1703, -0.1445],\n",
      "          [ 0.4908, -0.5639, -0.3677,  0.5085],\n",
      "          [-0.2217,  0.1626, -0.7850,  0.7893]]]],\n",
      "       grad_fn=<MkldnnConvolutionBackward>)\n",
      "tensor([[[[ 0.6956, -0.1893,  0.5630,  0.2856],\n",
      "          [ 0.1645,  0.5064, -0.2672, -0.2958],\n",
      "          [ 0.1333,  0.6983,  0.2453,  0.0849],\n",
      "          [ 0.7977,  0.0585,  0.8783, -0.2751]],\n",
      "\n",
      "         [[-0.0515, -0.2804, -0.3157,  0.5948],\n",
      "          [-0.5185,  0.1907,  1.2124, -0.3263],\n",
      "          [-0.3546, -0.2959,  0.1206,  0.1996],\n",
      "          [ 0.2462,  0.6708, -0.7042,  0.5976]],\n",
      "\n",
      "         [[ 0.4417,  0.5273,  0.4430,  0.5128],\n",
      "          [ 0.2436, -0.8946, -0.0797,  0.1172],\n",
      "          [ 0.5668,  0.3461,  0.1307,  0.2035],\n",
      "          [ 0.1604,  0.5501,  0.5731,  0.0167]],\n",
      "\n",
      "         [[-0.5004,  0.0868,  0.6181,  0.0332],\n",
      "          [-0.5029, -0.3809,  0.5123,  0.0351],\n",
      "          [-0.4592, -0.1736, -0.5052,  1.1377],\n",
      "          [-0.0333, -0.5458, -0.2187,  0.3438]],\n",
      "\n",
      "         [[-0.1665,  0.8854, -0.3702, -0.5449],\n",
      "          [-0.1738, -0.3198,  0.2138,  0.3196],\n",
      "          [ 0.3985,  0.4793,  0.0230,  0.1712],\n",
      "          [-0.4910,  0.3222,  0.1348, -0.0708]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0531,  0.1461,  0.6232,  0.0718],\n",
      "          [-0.6251, -0.0193,  0.1624,  0.3255],\n",
      "          [ 0.4257,  0.4789,  0.7777, -0.6988],\n",
      "          [-0.4430, -0.6492,  0.1970, -0.5754]],\n",
      "\n",
      "         [[ 0.0762, -0.3786, -0.0747, -0.2344],\n",
      "          [ 0.6301, -0.6136,  0.0673, -0.6432],\n",
      "          [-0.1791, -0.0781, -0.0333,  0.9803],\n",
      "          [-0.0750,  0.6129,  0.3926,  0.2010]],\n",
      "\n",
      "         [[ 0.3694,  0.4786,  0.2073,  0.0356],\n",
      "          [-0.2059,  0.5221, -0.5048,  0.1516],\n",
      "          [ 0.6729,  0.7409, -0.7168,  0.4773],\n",
      "          [-0.7519,  0.6723, -0.5056,  0.5694]],\n",
      "\n",
      "         [[-0.0485, -0.3119, -0.0690, -0.1777],\n",
      "          [ 0.4202, -0.6374,  0.1486,  0.7112],\n",
      "          [ 0.4979, -0.3066, -0.0874,  0.0450],\n",
      "          [-0.2004,  0.9521, -0.6918,  0.0363]],\n",
      "\n",
      "         [[ 0.3600,  0.3155, -0.2562, -0.1546],\n",
      "          [-0.0132, -0.0780,  0.1703, -0.1445],\n",
      "          [ 0.4908, -0.5639, -0.3677,  0.5085],\n",
      "          [-0.2217,  0.1626, -0.7850,  0.7893]]]])\n"
     ]
    }
   ],
   "source": [
    "class InvConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InvConv, self).__init__()\n",
    "        self.if_init = False\n",
    "\n",
    "    def init_layer(self, old_layer):\n",
    "        self.old_weight = old_layer.weight.detach().cpu().clone().numpy()\n",
    "        self.inp_padding = old_layer.padding\n",
    "        \n",
    "        self.inp_size = list(old_layer.inp_size)[1:] # inp_size was obtained by hook\n",
    "        \n",
    "        \n",
    "        if old_layer.padding[-1]>0:\n",
    "            self.inp_size[-1] += 2*old_layer.padding[-1]\n",
    "        if old_layer.padding[-2]>0:\n",
    "            self.inp_size[-2] += 2*old_layer.padding[-2]\n",
    "            \n",
    "        print('inp_size:', self.inp_size)\n",
    "        print('old weight', self.old_weight.shape)\n",
    "        self.old_weight_matrix = toeplitz_mult_ch(\n",
    "            self.old_weight, self.inp_size)\n",
    "        self.old_weight_matrix = torch.tensor(self.old_weight_matrix).float()\n",
    "        \n",
    "        \n",
    "        self.if_init = True\n",
    "        \n",
    "    def forward(self, y, if_inv=True):\n",
    "        '''\n",
    "        Problem: \n",
    "            1. the converted weight matrix is super large. This matrix may consum ~150G memory. Please do NOT use cuda. \n",
    "        '''\n",
    "        \n",
    "        assert self.if_init\n",
    "\n",
    "        # remember to transpose the old_weight_matrix\n",
    "        self.old_weight_matrix = self.old_weight_matrix.to(y.device)\n",
    "            \n",
    "        if if_inv:\n",
    "            '''\n",
    "            inversion of convolution\n",
    "            '''\n",
    "            \n",
    "            out = torch.matmul(self.old_weight_matrix.t()[None,:,:], y.view(y.size(0), -1)[:,:,None])\n",
    "\n",
    "            # reshape the output\n",
    "            out = out.view([out.size(0)] + self.inp_size)\n",
    "\n",
    "            # un-padding\n",
    "            if self.inp_padding[0] == 0 and self.inp_padding[1]==0:\n",
    "                out_unpadding = out\n",
    "            else:\n",
    "                out_unpadding = out[:,:,self.inp_padding[0]:-self.inp_padding[0],self.inp_padding[1]:-self.inp_padding[1]]\n",
    "\n",
    "            return out_unpadding\n",
    "        else:\n",
    "            '''\n",
    "            standard convolution. However, we implement the convolution with pure matrix(weights)-vector(input) multiplication.\n",
    "            \n",
    "            '''\n",
    "            x = y\n",
    "            x_pad = F.pad(x, pad=[self.inp_padding[0],self.inp_padding[0],self.inp_padding[1],self.inp_padding[1]])\n",
    "            print('### x_pad', x_pad.view(x_pad.size(0), -1)[:,:,None].size())\n",
    "            out = torch.matmul(self.old_weight_matrix[None,:,:], x_pad.view(x_pad.size(0), -1)[:,:,None])\n",
    "            # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=conv#torch.nn.Conv2d\n",
    "            out_size = int( (self.inp_size[-1] - 1 * (self.old_weight.shape[-1] -1) - 1)  / 1. + 1)\n",
    "            out = out.view([out.size(0), self.old_weight.shape[0]]+[out_size]*2)\n",
    "            return out\n",
    "\n",
    "    \n",
    "if 1:\n",
    "    # verify correction of our implementation\n",
    "    inp = torch.randn(2,3,4,4)\n",
    "    conv = nn.Conv2d(3, 5, 3, stride=1, padding=1, bias=False)\n",
    "    conv.inp_size = [2,3,4,4]\n",
    "    inv_conv = InvConv()\n",
    "    inv_conv.init_layer(conv)\n",
    "    \n",
    "    nn_ref = conv(inp)\n",
    "    print('nn Ref shape:', ref.shape)\n",
    "    f_ref = F.conv2d(inp, conv.weight, stride=1, padding=1)\n",
    "    out = inv_conv(inp, if_inv=False)\n",
    "    \n",
    "    print( (nn_ref-out).abs().max())\n",
    "    print( (f_ref-out).abs().max())\n",
    "    print(nn_ref)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input = torch.arange(16).reshape(1, 1, 4, 4).float()\n",
    "# print('\\ninput', input)\n",
    "\n",
    "# downsample = nn.Conv2d(1, 1, 2, stride=2, padding=0, bias=False)\n",
    "# downsample.weight.data.copy_(torch.tensor([[[[0.,1.],[3.,2.]]]]))\n",
    "\n",
    "# print('\\nkernel:', downsample.weight.data)\n",
    "\n",
    "# h = downsample(input)\n",
    "# print('\\nh:', h.data)\n",
    "\n",
    "# upsample = nn.ConvTranspose2d(1, 1, 2, stride=2, padding=0, bias=False)\n",
    "# upsample.weight.data.copy_(torch.tensor([[[[1.,1.],[1.,1.]]]])/6.)\n",
    "\n",
    "# output = upsample(h)\n",
    "# print('\\noutput:', output.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# input = torch.arange(32).reshape(1, 2, 4, 4).float()\n",
    "# print('\\ninput', input)\n",
    "\n",
    "# downsample = nn.Conv2d(2, 3, 2, stride=2, padding=0, bias=False)\n",
    "# downsample.weight.data.copy_(torch.arange(24).float().reshape(3,2,2,2))\n",
    "\n",
    "# print('\\nweight:', downsample.weight.data)\n",
    "# print('\\nweight size:', downsample.weight.data.size())\n",
    "\n",
    "# h = downsample(input)\n",
    "# print('\\nh:', h.data)\n",
    "\n",
    "# upsample = nn.ConvTranspose2d(3, 2, 2, stride=2, padding=0, bias=False)\n",
    "# print(upsample.weight.data.size())\n",
    "# upsample.weight.data.copy_(torch.ones(3,2,2,2))\n",
    "\n",
    "# output = upsample(h)\n",
    "# print('\\noutput:', output.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.307818\n",
      "1_Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.181390\n",
      "1_Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.432622\n",
      "1_Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.408617\n",
      "1_Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.288588\n",
      "1_Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.134501\n",
      "1_Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.113593\n",
      "1_Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.266873\n",
      "1_Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.214467\n",
      "1_Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.094832\n",
      "1_Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.190274\n",
      "1_Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.210753\n",
      "1_Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.114326\n",
      "1_Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.102974\n",
      "1_Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.142686\n",
      "1_Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.054878\n",
      "1_Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.168605\n",
      "1_Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.106960\n",
      "1_Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.191420\n",
      "1_Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.126642\n",
      "1_Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.066119\n",
      "1_Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.037918\n",
      "1_Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.034477\n",
      "1_Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.358834\n",
      "1_Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.064160\n",
      "1_Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.121926\n",
      "1_Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.189347\n",
      "1_Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.024076\n",
      "1_Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.110047\n",
      "1_Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.074207\n",
      "1_Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.087502\n",
      "1_Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.115496\n",
      "1_Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.014285\n",
      "1_Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.093092\n",
      "1_Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.002323\n",
      "1_Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.035697\n",
      "1_Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.064180\n",
      "1_Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.113947\n",
      "1_Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.003513\n",
      "1_Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.039149\n",
      "1_Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.024519\n",
      "1_Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.022159\n",
      "1_Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.227528\n",
      "1_Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.112838\n",
      "1_Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.037187\n",
      "1_Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.037762\n",
      "1_Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.024593\n",
      "1_Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.051660\n",
      "1_Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.008995\n",
      "1_Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.032274\n",
      "1_Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.142027\n",
      "1_Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.035760\n",
      "1_Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.096902\n",
      "1_Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.015626\n",
      "1_Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.011303\n",
      "1_Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.120297\n",
      "1_Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.106678\n",
      "1_Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.013795\n",
      "1_Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.032767\n",
      "1_Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.089130\n",
      "1_Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.103708\n",
      "1_Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.001645\n",
      "1_Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.037125\n",
      "1_Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.073875\n",
      "1_Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.071512\n",
      "1_Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.045259\n",
      "1_Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.013967\n",
      "1_Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.140145\n",
      "1_Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.095694\n",
      "1_Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.012598\n",
      "1_Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.045814\n",
      "1_Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.058183\n",
      "1_Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.214008\n",
      "1_Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.082269\n",
      "1_Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.068929\n",
      "1_Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.056930\n",
      "1_Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.006475\n",
      "1_Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.002886\n",
      "1_Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.025798\n",
      "1_Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.024242\n",
      "1_Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.140569\n",
      "1_Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.013753\n",
      "1_Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.030795\n",
      "1_Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.147803\n",
      "1_Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.064418\n",
      "1_Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.010761\n",
      "1_Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.004777\n",
      "1_Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.067144\n",
      "1_Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.007366\n",
      "1_Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.066697\n",
      "1_Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.064876\n",
      "1_Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.012288\n",
      "1_Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.002034\n",
      "1_Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.002908\n",
      "\n",
      "1_Test set: Average loss: 0.0467, Accuracy: 9840/10000 (98%)\n",
      "\n",
      "inp_size: [1, 30, 30]\n",
      "old weight (32, 1, 3, 3)\n",
      "==> output_size (32, 28, 28)\n",
      "==> T (32, 784, 1, 900)\n",
      "inp_size: [32, 16, 16]\n",
      "old weight (64, 32, 3, 3)\n",
      "==> output_size (64, 14, 14)\n",
      "==> T (64, 196, 32, 256)\n",
      "--------------------------------------\n",
      "runnning one batch inversion with direct inversion\n",
      "training with KD loss\n",
      "will be tested on ORIGINAL images\n",
      "NOISE_INPUT:  False\n",
      "**************************************\n",
      "training a inv_model to correctly excite the model\n",
      "displaying original input-inverted input pairs\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-68b92e7aa43d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def loss_fn_kd(outputs, teacher_outputs):\n",
    "    \"\"\"\n",
    "    Compute the knowledge-distillation (KD) loss given outputs\n",
    "    \"\"\"\n",
    "    T = 3.0\n",
    "    kld_loss = nn.KLDivLoss()(F.log_softmax(outputs / T, dim=1), F.softmax(teacher_outputs / T, dim=1))\n",
    "    KD_loss = kld_loss * T * T\n",
    "    return KD_loss\n",
    "\n",
    "\n",
    "\n",
    "class InvertNet(nn.Module):\n",
    "    def __init__(self, n_input_ch=1):\n",
    "        super(InvertNet, self).__init__()\n",
    "        # self.Tconv1 = nn.ConvTranspose2d(32, n_input_ch, 3, 1, bias=False)\n",
    "        # self.Tconv2 = nn.ConvTranspose2d(64, 32, 3, 1, bias=False)\n",
    "        self.Tconv1 = InvConv()\n",
    "        self.Tconv2 = InvConv()\n",
    "        self.Tfc1 = nn.Linear(128, int(64*7*7), bias=False)\n",
    "        self.Tfc2 = nn.Linear(10, 128, bias=False)\n",
    "        self.unpool = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear') #bilinear, nearest\n",
    "\n",
    "    def forward(self, output, ind1, ind2):\n",
    "        # going backward in the model\n",
    "        # assume we don't synthesize anything that was lost by nonlinarities, like relu.\n",
    "        # later we should add some noise like negative values to pre relu forexample\n",
    "        # we cheat here by using ind input, it can be randomly assigned or use a predefined pattern\n",
    "        # pdb.set_trace()\n",
    "        x = self.Tfc2(output)\n",
    "        # torch.Size([64, 128])\n",
    "        x = self.Tfc1(x)\n",
    "        # torch.Size([64, 9216])\n",
    "        #unflatten\n",
    "        x = x.reshape([64, 64, 7, 7])\n",
    "        if 1:\n",
    "            x = self.unpool(x, indices=ind2, output_size=torch.Size([64, 64, 14, 14]))\n",
    "        else:\n",
    "            x = self.upsample(x)\n",
    "        # torch.Size([64, 64, 24, 24])\n",
    "        x = self.Tconv2(x)\n",
    "        # torch.Size([64, 32, 26, 26])\n",
    "        x = self.unpool(x, indices=ind1, output_size=torch.Size([64, 32, 28, 28]))\n",
    "        x = self.Tconv1(x)\n",
    "        # torch.Size([64, 1, 28, 28])\n",
    "        return x\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_input_ch=3):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(n_input_ch, 32, 3, 1, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, padding=1, bias=False)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(int(64*7*7), 128, bias=False)\n",
    "        self.fc2 = nn.Linear(128, 10, bias=False)\n",
    "        self.pool = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        NO_DROP = True\n",
    "        # pdb.set_trace()\n",
    "        # x.shape : torch.Size([64, 1, 28, 28])\n",
    "        x = self.conv1(x)\n",
    "        # x.shape : torch.Size([64, 32, 28, 28])\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x, ind1 = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        # x.shape : torch.Size([64, 64, 14, 14])\n",
    "        x = F.relu(x)\n",
    "        # x, ind = F.max_pool2d(x, 2)\n",
    "        x, ind2 = self.pool(x)\n",
    "        # x.shape : torch.Size([64, 64, 7, 7])\n",
    "        # ind.indeces : torch.Size([64, 64, 7, 7])\n",
    "        # 0..22:2, 48..70:2 etc\n",
    "        if not NO_DROP:\n",
    "            x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # x.shape torch.Size([64, 9216])\n",
    "        x = self.fc1(x)\n",
    "        # x.shape torch.Size([64, 128])\n",
    "        x = F.relu(x)\n",
    "        if not NO_DROP:\n",
    "            x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        # x.shape torch.Size([64, 10])\n",
    "        features = x\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output, (features, ind1, ind2)\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('1_Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, _ = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\n1_Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=1, metavar='N',\n",
    "                    help='number of epochs to train (default: 14)')\n",
    "parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                    help='learning rate (default: 1.0)')\n",
    "parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                    help='Learning rate step gamma (default: 0.7)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=True,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                    help='quickly check a single pass')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                    help='For Saving the current Model')\n",
    "parser.add_argument('--input_noise', action='store_true', default=False,\n",
    "                    help='learn distillation model from noise and not inverted images')\n",
    "parser.add_argument('--cifar', action='store_true', default=False,\n",
    "                    help='Use CIFAR10 dataset, if not set then use MNIST')\n",
    "parser.add_argument('--train_inv_model', action='store_true', default=False,\n",
    "                    help='Train inv_model so that output of it is correctly classified with model')\n",
    "args = parser.parse_args('')\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'batch_size': args.batch_size}\n",
    "if use_cuda:\n",
    "    kwargs.update({'num_workers': 1,\n",
    "                    'pin_memory': True,\n",
    "                    'shuffle': True},\n",
    "                    )\n",
    "\n",
    "CIFAR = args.cifar\n",
    "if CIFAR:\n",
    "    n_input_ch = 3\n",
    "else:\n",
    "    n_input_ch = 1\n",
    "\n",
    "if not args.cifar:\n",
    "    #MNIST\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                        transform=transform)\n",
    "    dataset2 = datasets.MNIST('../data', train=False,\n",
    "                        transform=transform)\n",
    "else:\n",
    "    #CIFAR10\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.RandomCrop(28),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "    dataset1 = datasets.CIFAR10('../data', train=True, download=True,\n",
    "                                transform=transform)\n",
    "    dataset2 = datasets.CIFAR10('../data', train=False,\n",
    "                                transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **kwargs)\n",
    "\n",
    "model = Net(n_input_ch=n_input_ch).to(device)\n",
    "\n",
    "# torch.nn.init.normal_(model.conv1.weight) \n",
    "# torch.nn.init.normal_(model.conv2.weight) \n",
    "# torch.nn.init.normal_(model.fc1.weight) \n",
    "# torch.nn.init.normal_(model.fc2.weight) \n",
    "\n",
    "\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "# if args.save_model:\n",
    "#     torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "inv_model = InvertNet(n_input_ch=n_input_ch).to(device)\n",
    "# pdb.set_trace()\n",
    "\n",
    "model.conv1.inp_size = [64, 1, 28, 28]\n",
    "model.conv2.inp_size = [64, 32, 14, 14]\n",
    "\n",
    "# inv_model.Tconv1.weight.data = torch.ones_like(model.conv1.weight.data) / model.conv1.weight.data.sum(dim=[2,3], keepdim=True)\n",
    "# inv_model.Tconv2.weight.data = torch.ones_like(model.conv2.weight.data) / model.conv2.weight.data.sum(dim=[2,3], keepdim=True) \n",
    "inv_model.Tconv1.init_layer(model.conv1)\n",
    "inv_model.Tconv2.init_layer(model.conv2)\n",
    "\n",
    "# inv_model.Tfc1.weight.data = model.fc1.weight.data.T.contiguous().clone()\n",
    "# inv_model.Tfc2.weight.data = model.fc2.weight.data.T.contiguous().clone()\n",
    "inv_model.Tfc1.weight.data = torch.pinverse(model.fc1.weight.data.T).T\n",
    "inv_model.Tfc2.weight.data = torch.pinverse(model.fc2.weight.data.T).T\n",
    "print(\"--------------------------------------\")\n",
    "print(\"runnning one batch inversion with direct inversion\")\n",
    "\n",
    "\n",
    "TRAIN_ON_INVERTED = False\n",
    "# NOISE_INPUT = True\n",
    "NOISE_INPUT = args.input_noise\n",
    "if TRAIN_ON_INVERTED:\n",
    "    print(\"printing with target class labels for inverted images\")\n",
    "    print(\"will be tested on INVERTED images\")\n",
    "else:\n",
    "    print(\"training with KD loss\")\n",
    "    print(\"will be tested on ORIGINAL images\")\n",
    "\n",
    "print(\"NOISE_INPUT: \", NOISE_INPUT)\n",
    "\n",
    "\n",
    "print(\"**************************************\")\n",
    "print('training a inv_model to correctly excite the model')\n",
    "if args.train_inv_model:\n",
    "    # optimizer_inv_model = optim.Adam(inv_model.parameters(), lr=1e-4)\n",
    "    optimizer_inv_model = optim.Adadelta(inv_model.parameters(), lr=args.lr)\n",
    "    scheduler = StepLR(optimizer_inv_model, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        ###### training on inverted images, KD will work better\n",
    "        inv_model.train()\n",
    "        model.train()\n",
    "        correct_forward_inverse= 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer_inv_model.zero_grad()\n",
    "            model.zero_grad()\n",
    "\n",
    "            if data.shape[0] != 64:\n",
    "                print(\"hardcoded batch size 64 only\")\n",
    "                continue\n",
    "\n",
    "            output_teacher, (features_teacher, ind) = model(data)\n",
    "            #generate input\n",
    "            inv_input = inv_model(features_teacher.detach(), ind)\n",
    "\n",
    "            output, (features, _) = model(inv_input)\n",
    "            # loss = F.nll_loss(output, target)\n",
    "            loss = loss_fn_kd(features, features_teacher)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer_inv_model.step()\n",
    "\n",
    "            correct_forward_inverse += (output.argmax(dim=1) == target).sum() / float(target.numel())\n",
    "\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                print('2a_Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tCorrect forward of inverted {:.1f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                            100. * batch_idx / len(train_loader), loss.item(), 100. * correct_forward_inverse / float(batch_idx+1) ))\n",
    "        ### testing:\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output, (features, ind1, ind2) = model(data)\n",
    "\n",
    "    inv_input = inv_model(features, ind1, ind2)\n",
    "    if NOISE_INPUT:\n",
    "        # generate random input by perturbing all pixels\n",
    "        idx = torch.randperm(data.nelement())\n",
    "        inv_input = data.view(-1)[idx].view(data.size())\n",
    "    # pdb.set_trace()\n",
    "    # import numpy as np\n",
    "    # data_display = np.concatenate((data.data.cpu().numpy(),  inv_input.data.cpu().numpy()), 3)\n",
    "    data_display = torch.cat((data,  inv_input), 0)\n",
    "    print(\"displaying original input-inverted input pairs\")\n",
    "    vutils.save_image(data_display,'orig_inverted_ones.png', normalize=True, scale_each=True, nrow=int(8))\n",
    "\n",
    "    break\n",
    "\n",
    "assert 1==0\n",
    "\n",
    "print(\"--------------------------------------\")\n",
    "print(\"training calssifier on INVERTED images\")\n",
    "\n",
    "\n",
    "model_on_inv = Net(n_input_ch=n_input_ch).to(device)\n",
    "optimizer = optim.Adadelta(model_on_inv.parameters(), lr=args.lr)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    ###### training on inverted images, KD will work better\n",
    "    model_on_inv.train()\n",
    "    model.train()\n",
    "    correct_forward_inverse= 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        inv_model.zero_grad()\n",
    "        model_on_inv.zero_grad()\n",
    "\n",
    "        if data.shape[0] != 64:\n",
    "            print(\"hardcoded batch size 64 only\")\n",
    "            continue\n",
    "\n",
    "        output_teacher, (features_teacher, ind) = model(data)\n",
    "        #generate input\n",
    "        inv_input = inv_model(features_teacher, ind)\n",
    "\n",
    "        if NOISE_INPUT:\n",
    "            #generate random input by perturbing all pixels\n",
    "            idx = torch.randperm(data.nelement())\n",
    "            inv_input = data.view(-1)[idx].view(data.size())\n",
    "\n",
    "        output, (features, _) = model_on_inv(inv_input)\n",
    "        if TRAIN_ON_INVERTED:\n",
    "            loss = F.nll_loss(output, target)\n",
    "        else:\n",
    "            output_teacher, (features_teacher, ind) = model(inv_input)\n",
    "            # Correctly classified images: (output_teacher.argmax(dim=1) == target).sum() / float(target.numel())\n",
    "            correct_forward_inverse += (output_teacher.argmax(dim=1) == target).sum() / float(target.numel())\n",
    "            loss = loss_fn_kd(features, features_teacher)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('3_Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tCorrect forward of inverted {:.1f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader), loss.item(), 100. * correct_forward_inverse / float(batch_idx+1) ))\n",
    "    ### testing:\n",
    "    model_on_inv.eval()\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            if data.shape[0] != 64:\n",
    "                print(\"hardcoded batch size 64 only\")\n",
    "                continue\n",
    "\n",
    "            if TRAIN_ON_INVERTED:\n",
    "                output_teacher, (features, ind) = model(data)\n",
    "                inv_input = inv_model(features, ind)\n",
    "                output, _ = model_on_inv(inv_input)\n",
    "                test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            else:\n",
    "                output_teacher, (features_teacher, _) = model(data)\n",
    "                output, (features, _) = model_on_inv(data)\n",
    "                test_loss += loss_fn_kd(features, features_teacher)\n",
    "\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\n3_Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    # %98 accuracy\n",
    "\n",
    "\n",
    "\n",
    "    scheduler.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
