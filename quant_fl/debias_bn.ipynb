{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Len of last client: 10000\n",
      "(Device 0) Train Loss: 1.206 | Train Acc: 56.600\n",
      "-------------------- 2000 ------------------\n",
      "R_feature: 4.328e+00, R_cross: 1.338e+00, R_total: 4.463e+01\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'inb_target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d6e7a9a5348a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    189\u001b[0m                                               reset_opt=True)\n\u001b[1;32m    190\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inv_image'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minv_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mdevice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inv_target'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minb_target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mall_inv_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inb_target' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "# import matplotlib.pyplot as plt\n",
    "import model\n",
    "import fl_data\n",
    "import quant\n",
    "import utils\n",
    "import agg\n",
    "from di import DeepInversionClass\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter('./test')\n",
    "nbit = 32\n",
    "rounds = 100\n",
    "\n",
    "num_devices = 2\n",
    "device_pct = 1.0\n",
    "\n",
    "local_epochs = 5\n",
    "local_lr = 0.01\n",
    "# global_lr = 0.05\n",
    "\n",
    "# Using CIFAR-10 again as in Assignment 1\n",
    "# Load training data\n",
    "transform_train = transforms.Compose([                                   \n",
    "    transforms.RandomCrop(32, padding=4),                                       \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "trainset = torchvision.datasets.CIFAR10(root='/cifar', train=True, \n",
    "                                        download=True,\n",
    "                                        transform=transform_train)\n",
    "\n",
    "# Load testing data\n",
    "transform_test = transforms.Compose([                                           \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "testset = torchvision.datasets.CIFAR10(root='/cifar', train=False,\n",
    "                                       download=True,\n",
    "                                       transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False,\n",
    "                                         num_workers=2)\n",
    "\n",
    "\n",
    "# Using same ConvNet as in Assignment 1\n",
    "\n",
    "\n",
    "def create_device(net, device_id, trainset, data_idxs, lr=0.1,\n",
    "                  milestones=None, batch_size=128):\n",
    "    if milestones == None:\n",
    "        milestones = [25, 50, 75]\n",
    "\n",
    "    device_net = copy.deepcopy(net)\n",
    "    optimizer = torch.optim.SGD(device_net.parameters(), lr=lr, momentum=0.9,\n",
    "                                weight_decay=5e-4)\n",
    "    # optimizer = torch.optim.Adam(device_net.parameters(), lr=lr, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                     milestones=milestones,\n",
    "                                                     gamma=0.1)\n",
    "    device_trainset = fl_data.DatasetSplit(trainset, data_idxs)\n",
    "    device_trainloader = torch.utils.data.DataLoader(device_trainset,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     shuffle=True,\n",
    "                                                     num_workers=8)\n",
    "    return {\n",
    "        'net': device_net,\n",
    "        'id': device_id,\n",
    "        'dataloader': device_trainloader, \n",
    "        'optimizer': optimizer,\n",
    "        'scheduler': scheduler,\n",
    "        'train_loss_tracker': [],\n",
    "        'train_acc_tracker': [],\n",
    "        'test_loss_tracker': [],\n",
    "        'test_acc_tracker': [],\n",
    "        'tb_writers': {'train_loss': utils.AutoStep(writer.add_scalar, 'client/%s/train_loss'%device_id),\n",
    "                       'train_acc': utils.AutoStep(writer.add_scalar, 'client/%s/train_acc'%device_id),\n",
    "                       'test_loss': utils.AutoStep(writer.add_scalar, 'client/%s/test_loss'%device_id),\n",
    "                       'test_acc' : utils.AutoStep(writer.add_scalar, 'client/%s/test_acc'%device_id)}\n",
    "        }\n",
    "  \n",
    "def train(epoch, device, tb=True):\n",
    "    device['net'].train()\n",
    "    train_loss, correct, total = 0, 0, 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(device['dataloader']):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        device['optimizer'].zero_grad()\n",
    "        outputs = device['net'](inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        device['optimizer'].step()\n",
    "        train_loss += loss.item()\n",
    "        device['train_loss_tracker'].append(loss.item())\n",
    "        loss = train_loss / (batch_idx + 1)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        acc = 100. * correct / total\n",
    "        dev_id = device['id']\n",
    "        # print(f'\\r(Device {dev_id}/Epoch {epoch}) ' + \n",
    "        #                  f'Train Loss: {loss:.3f} | Train Acc: {acc:.3f}')\n",
    "    if tb:\n",
    "        device['train_acc_tracker'].append(acc)\n",
    "        device['tb_writers']['train_loss'].write(loss)\n",
    "        device['tb_writers']['train_acc'].write(acc)\n",
    "    return loss, acc\n",
    "\n",
    "def test(epoch, device, tb=True):\n",
    "    device['net'].eval()\n",
    "    test_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = device['net'](inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            device['test_loss_tracker'].append(loss.item())\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            loss = test_loss / (batch_idx + 1)\n",
    "            acc = 100.* correct / total\n",
    "    # print(f' | Test Loss: {loss:.3f} | Test Acc: {acc:.3f}\\n')\n",
    "    acc = 100.*correct/total\n",
    "\n",
    "    if tb:\n",
    "        device['test_acc_tracker'].append(acc)\n",
    "        device['tb_writers']['test_loss'].write(loss)\n",
    "        device['tb_writers']['test_acc'].write(acc)\n",
    "    return loss, acc\n",
    "    \n",
    "\n",
    "def get_devices_for_round(devices, device_pct):\n",
    "    '''\n",
    "    '''\n",
    "    assert device_pct>0 and device_pct<=1, 'device pct must be in the range of (0,1].'\n",
    "    num_devices_in_round = round(device_pct*len(devices))\n",
    "    device_idxs = np.random.permutation(len(devices))[:num_devices_in_round]\n",
    "    return [devices[i] for i in device_idxs]\n",
    "\n",
    "def update_bn_stat(model, all_inv_image, invbn_epochs=50):\n",
    "    all_inv_image_tensor = torch.cat(all_inv_image)\n",
    "    model.train()\n",
    "    for epoch_idx in range(invbn_epochs):\n",
    "        outputs = model(all_inv_image_tensor)\n",
    "\n",
    "net = model.ConvNet().cuda()\n",
    "# net = model.CifarNet().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "data_idxs_dict = fl_data.uniform_random_split(trainset, num_devices)\n",
    "# deep copy net for each devices\n",
    "devices = [create_device(net, i, trainset, data_idxs_dict[i], lr=local_lr)\n",
    "           for i in range(num_devices)]\n",
    "\n",
    "# w_avg = net.state_dict()\n",
    "## IID Federated Learning\n",
    "inv_batchsize = 10\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "for round_num in range(rounds):\n",
    "    round_devices = get_devices_for_round(devices, device_pct)\n",
    "    all_inv_image = []\n",
    "    \n",
    "    for round_device_idx, device in enumerate(round_devices):\n",
    "        for local_epoch in range(local_epochs):\n",
    "            local_loss, local_acc = train(local_epoch, device)\n",
    "        print(f'\\r(Device {round_device_idx}) ' + \n",
    "                        f'Train Loss: {local_loss:.3f} | Train Acc: {local_acc:.3f}')\n",
    "        # after local updating, we invert some images. \n",
    "        # Notice, this can be done on the cloud\n",
    "        di = DeepInversionClass(bs=inv_batchsize, \n",
    "                                net_teacher=device['net'], path='./test')\n",
    "        inv_image, inv_target = di.get_images(net_student=None, \n",
    "                                              reset_inputs=True, \n",
    "                                              reset_targets=True, \n",
    "                                              reset_opt=True,\n",
    "                                              iterations_per_layer=2000)\n",
    "        device['inv_image'] = inv_image\n",
    "        device['inv_target'] = inv_target \n",
    "        all_inv_image.append(inv_image)\n",
    "        \n",
    "    new_w_avg = agg.average_weights(round_devices) # average all in the state_dict\n",
    "    net.load_state_dict(new_w_avg)\n",
    "    \n",
    "    update_bn_stat(net, all_inv_image, invbn_epochs=50)\n",
    "    w_avg = net.state_dict()\n",
    "    \n",
    "\n",
    "    for device in devices:\n",
    "        device['net'].load_state_dict(w_avg)\n",
    "        device['optimizer'].zero_grad()\n",
    "        device['optimizer'].step()\n",
    "        device['scheduler'].step()\n",
    "\n",
    "    # test accuracy after aggregation\n",
    "    round_loss, round_acc = test(round_num, devices[0], tb=False)\n",
    "    writer.add_scalar('round/loss', round_loss, round_num)\n",
    "    writer.add_scalar('round/acc', round_acc, round_num)\n",
    "    print('====> Round:%d, Acc:%.4f'%(round_num, round_acc))\n",
    "\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print('Total training time: {} seconds'.format(total_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
