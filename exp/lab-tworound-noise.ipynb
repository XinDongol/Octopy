{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 17})\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Using CIFAR-10 again as in Assignment 1\n",
    "# Load training data\n",
    "transform_train = transforms.Compose([                                   \n",
    "    transforms.RandomCrop(32, padding=4),                                       \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# data_dir = '/home/jovyan/harvard-heavy/xin/cifar10_data/'\n",
    "data_dir = '/n/holyscratch01/kung_lab/xin/cifar_data'\n",
    "trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True, \n",
    "                                        download=True,\n",
    "                                        transform=transform_train)\n",
    "\n",
    "# Load testing data\n",
    "transform_test = transforms.Compose([                                           \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "testset = torchvision.datasets.CIFAR10(root=data_dir, train=False,\n",
    "                                       download=True,\n",
    "                                       transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False,\n",
    "                                         num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using same ConvNet as in Assignment 1\n",
    "'''\n",
    "def conv_block(in_channels, out_channels, kernel_size=3, stride=1,\n",
    "               padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,\n",
    "                  bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            conv_block(3, 32),\n",
    "            conv_block(32, 32),\n",
    "            conv_block(32, 64, stride=2),\n",
    "            conv_block(64, 64),\n",
    "            conv_block(64, 64),\n",
    "            conv_block(64, 128, stride=2),\n",
    "            conv_block(128, 128),\n",
    "            conv_block(128, 256),\n",
    "            conv_block(256, 256),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "            )\n",
    "\n",
    "        self.classifier = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.model(x)\n",
    "        B, C, _, _ = h.shape\n",
    "        h = h.view(B, C)\n",
    "        return self.classifier(h)\n",
    "'''\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    '''\n",
    "    From https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/cifar10_tutorial.py\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvNet().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "net.train()\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9,\n",
    "                            weight_decay=5e-4)\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    loss.backward(retain_graph=True, create_graph=True)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 400])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for group in optimizer.param_groups:\n",
    "    for param in group['params']:\n",
    "        print(param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = F.l1_loss(net.conv1.weight.grad, torch.zeros_like(net.conv1.weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSplit(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return image, torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for test\n",
    "# test_corr = DatasetSplit(testset, [1,2,3])\n",
    "# len(test_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, device):\n",
    "    net.train()\n",
    "    train_loss, correct, total = 0, 0, 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(device['dataloader']):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        device['optimizer'].zero_grad()\n",
    "        outputs = device['net'](inputs)\n",
    "        \n",
    "        if isinstance(device['criterion'], nn.CrossEntropyLoss):\n",
    "            loss = device['criterion'](outputs, targets)\n",
    "        else:\n",
    "            loss, data_loss, fisher_loss = device['criterion'](outputs, targets, False)\n",
    "            device['train_data_loss_tracker'].append(data_loss.item())\n",
    "            device['train_fisher_loss_tracker'].append(fisher_loss.item())\n",
    "            \n",
    "        loss.backward()\n",
    "        # nn.utils.clip_grad_norm_(device['net'].parameters(), max_norm = 1)\n",
    "        nn.utils.clip_grad_value_(device['net'].parameters(), clip_value=5)\n",
    "        \n",
    "        device['optimizer'].step()\n",
    "        train_loss += loss.item()\n",
    "        device['train_loss_tracker'].append(loss.item())\n",
    "        avg_loss = train_loss / (batch_idx + 1)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        acc = 100. * correct / total\n",
    "        dev_id = device['id']\n",
    "        \n",
    "    print('\\n'+' '*30)\n",
    "    if isinstance(device['criterion'], nn.CrossEntropyLoss):\n",
    "        sys.stdout.write(f'\\r(Device {dev_id}/Epoch {epoch}) ' + \n",
    "                         f'Train Loss: {avg_loss:.3f} | Train Acc: {acc:.3f}')\n",
    "    else:\n",
    "        sys.stdout.write(f'\\r(Device {dev_id}/Epoch {epoch}) ' + \n",
    "                         f'Train Loss: {avg_loss:.3f} Data: {data_loss.item():.3f} Fisher: {fisher_loss.item():.6f} | Train Acc: {acc:.3f}')\n",
    "    sys.stdout.flush()\n",
    "    device['train_acc_tracker'].append(acc)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def test(epoch, device):\n",
    "    net.eval()\n",
    "    test_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = device['net'](inputs)\n",
    "            \n",
    "            if isinstance(device['criterion'], nn.CrossEntropyLoss):\n",
    "                loss = device['criterion'](outputs, targets)\n",
    "            else:\n",
    "                loss, data_loss, fisher_loss = device['criterion'](outputs, targets, False)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            avg_loss = test_loss / (batch_idx + 1)\n",
    "            acc = 100.* correct / total\n",
    "            \n",
    "    device['test_loss_tracker'].append(avg_loss)\n",
    "\n",
    "    if isinstance(device['criterion'], nn.CrossEntropyLoss):\n",
    "        sys.stdout.write(f' | Test Loss: {avg_loss:.3f} | Test Acc: {acc:.3f}\\n')\n",
    "    else:\n",
    "        sys.stdout.write(f' | Test Loss: {avg_loss:.3f}, Data: {data_loss.item():.8f}, Reg: {fisher_loss.item():.8f} | Test Acc: {acc:.3f}\\n')\n",
    "    sys.stdout.flush()  \n",
    "    acc = 100.*correct/total\n",
    "    device['test_acc_tracker'].append(acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_device(net, device_id, trainset, data_idxs, criterion, lr=0.01,\n",
    "                  milestones=None, batch_size=32):\n",
    "    if milestones == None:\n",
    "        milestones = [25, 50, 75]\n",
    "\n",
    "    device_net = copy.deepcopy(net)\n",
    "    optimizer = torch.optim.SGD(device_net.parameters(), lr=lr, momentum=0.9,\n",
    "                                weight_decay=5e-4)\n",
    "#     optimizer = torch.optim.Adam(device_net.parameters(), lr=lr,\n",
    "#                                 weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                     milestones=milestones,\n",
    "                                                     gamma=0.1)\n",
    "    device_trainset = DatasetSplit(trainset, data_idxs)\n",
    "    device_trainloader = torch.utils.data.DataLoader(device_trainset,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     shuffle=True,\n",
    "                                                     num_workers=2)\n",
    "    return {\n",
    "        'net': device_net,\n",
    "        'id': device_id,\n",
    "        'dataloader': device_trainloader, \n",
    "        'optimizer': optimizer,\n",
    "        'scheduler': scheduler,\n",
    "        'criterion': criterion,\n",
    "        'train_loss_tracker': [],\n",
    "        'train_data_loss_tracker': [],\n",
    "        'train_fisher_loss_tracker': [],\n",
    "        'train_acc_tracker': [],\n",
    "        'test_loss_tracker': [],\n",
    "        'test_data_loss_tracker': [],\n",
    "        'test_fisher_loss_tracker': [],\n",
    "        'test_acc_tracker': [],\n",
    "        }\n",
    "    \n",
    "def iid_sampler(dataset, num_devices, data_pct):\n",
    "    '''\n",
    "    dataset: PyTorch Dataset (e.g., CIFAR-10 training set)\n",
    "    num_devices: integer number of devices to create subsets for\n",
    "    data_pct: percentage of training samples to give each device\n",
    "              e.g., 0.1 represents 10%\n",
    "\n",
    "    return: a dictionary of the following format:\n",
    "      {\n",
    "        0: [3, 65, 2233, ..., 22] // device 0 sample indexes\n",
    "        1: [0, 2, 4, ..., 583] // device 1 sample indexes\n",
    "        ...\n",
    "      }\n",
    "\n",
    "    iid (independent and identically distributed) means that the indexes\n",
    "    should be drawn independently in a uniformly random fashion.\n",
    "    '''\n",
    "\n",
    "    # total number of samples in the dataset\n",
    "    total_samples = len(dataset)\n",
    "\n",
    "    # Part 1.1: Implement!\n",
    "    device_samples = round(data_pct * total_samples)\n",
    "    return {i: torch.randperm(total_samples)[:device_samples].tolist() \n",
    "            for i in range(num_devices)}\n",
    "\n",
    "def noniid_sampler(dataset, num_users, num_shards_per_user=2):\n",
    "    \"\"\"\n",
    "    Sample non-I.I.D client data from dataset\n",
    "    \"\"\"\n",
    "    total_num_imgs = len(dataset)\n",
    "    num_shards = int(num_shards_per_user*num_users)\n",
    "    num_imgs = int(total_num_imgs/num_shards)\n",
    "\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {i: np.array([]) for i in range(num_users)}\n",
    "    idxs = np.arange(num_shards*num_imgs)\n",
    "    labels = np.array(dataset.targets)\n",
    "\n",
    "    # sort labels\n",
    "    idxs_labels = np.vstack((idxs, labels))\n",
    "    # idxs_labels[1, :].argsort(): sort idxs by classes\n",
    "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()].astype(int)\n",
    "    # idxs_labels:\n",
    "    # array([[30207,  5662, 55366, ..., 23285, 15728, 11924],\n",
    "    #        [    0,     0,     0, ...,     9,     9,     9]])\n",
    "    idxs = idxs_labels[0, :]\n",
    "\n",
    "    # divide and assign 2 shards/client\n",
    "    for i in range(num_users):\n",
    "        rand_set = set(np.random.choice(idx_shard, num_shards_per_user, replace=False))\n",
    "        idx_shard = list(set(idx_shard) - rand_set)\n",
    "        for rand in rand_set:\n",
    "            dict_users[i] = np.concatenate(\n",
    "                (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0).astype(int)\n",
    "    return dict_users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_matrix_diag(device, batch_size=None, empr=False):\n",
    "    # Init\n",
    "    model = device['net']\n",
    "    batch_size = len(device['dataloader'].dataset) if batch_size is None else batch_size\n",
    "    dataloader =  torch.utils.data.DataLoader(device['dataloader'].dataset,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     shuffle=True,\n",
    "                                                     num_workers=0)\n",
    "    fisher={}\n",
    "    for n,p in model.named_parameters():\n",
    "        fisher[n]=torch.zeros_like(p.data).detach()\n",
    "    # Compute\n",
    "    model.eval()\n",
    "    total_num_samples = 0\n",
    "    for image, target in dataloader:\n",
    "        image, target = image.cuda(), target.cuda()\n",
    "        total_num_samples += image.size(0)\n",
    "        # Forward and backward\n",
    "        model.zero_grad()\n",
    "        output=model.forward(image)\n",
    "        if empr:\n",
    "            target = output.max(1)[1]\n",
    "        loss=criterion(output, target)\n",
    "        loss.backward()\n",
    "        # Get gradients\n",
    "        for n,p in model.named_parameters():\n",
    "            if p.grad is not None:\n",
    "                fisher[n] += image.size(0) * (p.grad.detach() ** 2)\n",
    "                \n",
    "    for n in fisher.keys():\n",
    "        fisher[n] /= total_num_samples\n",
    "        fisher[n].requires_grad=False\n",
    "    return fisher\n",
    "\n",
    "\n",
    "def mas_matrix(device, batch_size=None):\n",
    "    # Init\n",
    "    model = device['net']\n",
    "    # batch_size = len(device['dataloader'].dataset) if batch_size is None else batch_size\n",
    "    assert batch_size is not None\n",
    "    \n",
    "    dataloader =  torch.utils.data.DataLoader(device['dataloader'].dataset,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     shuffle=True,\n",
    "                                                     num_workers=0)\n",
    "    fisher={}\n",
    "    for n,p in model.named_parameters():\n",
    "        fisher[n]=torch.zeros_like(p.data).detach()\n",
    "    # Compute\n",
    "    model.eval()\n",
    "    total_num_samples = 0\n",
    "    \n",
    "    target = None\n",
    "    loss_func = torch.nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    for idx, (image, _) in enumerate(dataloader):\n",
    "        image = image.cuda()\n",
    "        total_num_samples += image.size(0)\n",
    "        assert image.size(0) == batch_size\n",
    "        # Forward and backward\n",
    "        model.zero_grad()\n",
    "        output=model.forward(image)\n",
    "\n",
    "        if target is None:\n",
    "            target = torch.zeros_like(output, requires_grad=False)\n",
    "        \n",
    "        loss=loss_func(output, target)\n",
    "        loss.backward()\n",
    "        # Get gradients\n",
    "        for n,p in model.named_parameters():\n",
    "            if p.grad is not None:\n",
    "                fisher[n] = fisher[n].mul(idx*batch_size) \n",
    "                fisher[n] = fisher[n].add(p.grad.data.clone().abs_())\n",
    "                fisher[n] = fisher[n].div((idx+1)*batch_size)\n",
    "                \n",
    "    return fisher\n",
    "\n",
    "\n",
    "'''\n",
    "def update_fisher_params(self, current_ds, batch_size, num_batch=None):\n",
    "    dl = DataLoader(current_ds, batch_size, shuffle=True)\n",
    "    log_liklihoods = []\n",
    "    for i, (input, target) in enumerate(dl):\n",
    "        if num_batch is not None:\n",
    "            if i > num_batch:\n",
    "                break\n",
    "        output = F.log_softmax(self.model(input), dim=1)\n",
    "        log_liklihoods.append(output[:, target])\n",
    "    log_likelihood = torch.cat(log_liklihoods).mean()\n",
    "    grad_log_liklihood = autograd.grad(log_likelihood, self.model.parameters())\n",
    "    _buff_param_names = [param[0].replace('.', '__') for param in self.model.named_parameters()]\n",
    "    for _buff_param_name, param in zip(_buff_param_names, grad_log_liklihood):\n",
    "        self.model.register_buffer(_buff_param_name+'_estimated_fisher', param.data.clone() ** 2)\n",
    "'''       \n",
    "\n",
    "def get_fisher_criterion(criterion, fisher, model, model_old, lamb):\n",
    "    # Regularization for all previous tasks\n",
    "    def fisher_criterion(output, target, ifprint=True):\n",
    "        loss_reg=0\n",
    "        for (name,param),(name_old,param_old) in zip(model.named_parameters(), model_old.named_parameters()):\n",
    "            assert name==name_old\n",
    "            loss_reg+=torch.sum(fisher[name].detach()*((param_old.detach()-param)**2))\n",
    "            \n",
    "        data_loss = criterion(output, target)\n",
    "        \n",
    "        if ifprint:\n",
    "            print('\\n N: %f, R: %f' % (data_loss.item(), lamb*loss_reg.item()))\n",
    "\n",
    "        return data_loss + lamb*loss_reg, data_loss, lamb*loss_reg\n",
    "\n",
    "    return fisher_criterion\n",
    "\n",
    "\n",
    "def get_l2_criterion(criterion, model, model_old, lamb):\n",
    "    # Regularization for all previous tasks\n",
    "    def l2_criterion(output, target, ifprint=True):\n",
    "        loss_reg=0\n",
    "        for (name,param),(name_old,param_old) in zip(model.named_parameters(), model_old.named_parameters()):\n",
    "            assert name==name_old\n",
    "            loss_reg+=torch.sum((param_old.detach()-param)**2)\n",
    "            \n",
    "        data_loss = criterion(output, target)\n",
    "            \n",
    "        if ifprint:\n",
    "            print('\\n N: %f, R: %f' % (data_loss.item(), lamb*loss_reg.item()))\n",
    "\n",
    "        return data_loss + lamb*loss_reg, data_loss, lamb*loss_reg\n",
    "\n",
    "    return l2_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weights(devices):\n",
    "    '''\n",
    "    Returns the average of the weights.\n",
    "    '''\n",
    "    # Part 1.2: Implement!\n",
    "    w = [device['net'].state_dict() for device in devices]\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key], len(w))\n",
    "        \n",
    "    return w_avg\n",
    "\n",
    "\n",
    "def zeros_like_copy(in_state_dict):\n",
    "    out_state_dict = {}\n",
    "    for key in in_state_dict.keys():\n",
    "        out_state_dict[key] = torch.zeros_like(in_state_dict[key])\n",
    "    return out_state_dict\n",
    "    \n",
    "\n",
    "def fisher_average_weights(devices):\n",
    "    w = [device['net'].state_dict() for device in devices]\n",
    "    fisher = [device['fisher'] for device in devices]\n",
    "    \n",
    "    w_avg = zeros_like_copy(w[0])\n",
    "    w_sum_fisher = zeros_like_copy(w[0])\n",
    "    \n",
    "    for key in w_avg.keys():\n",
    "        for i in range(len(w)):\n",
    "            try:\n",
    "                fisher_matrix = fisher[i][key] + 1e-8\n",
    "                # fisher_matrix = torch.ones_like(w[i][key])\n",
    "            except:\n",
    "                fisher_matrix = torch.ones_like(w[i][key])\n",
    "                \n",
    "            w_avg[key] += w[i][key]*fisher_matrix\n",
    "            w_sum_fisher[key] += fisher_matrix\n",
    "            \n",
    "        w_avg[key] /= w_sum_fisher[key]\n",
    "        \n",
    "    return w_avg\n",
    "    \n",
    "\n",
    "def get_devices_for_round(devices, device_pct):\n",
    "    '''\n",
    "    '''\n",
    "    # Part 1.2: Implement!\n",
    "    num_devices_in_round = round(device_pct*len(devices))\n",
    "    device_idxs = np.random.permutation(len(devices))[:num_devices_in_round]\n",
    "    return [devices[i] for i in device_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvNet().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Part 1.3: Implement device creation here\n",
    "# devices = [create_device(net, i, trainset, data_idxs[i])\n",
    "#            for i in range(num_devices)]\n",
    "fisher_batch_size = 1000\n",
    "\n",
    "number_center_devices = 6\n",
    "\n",
    "center_list = [create_device(net, 50000, trainset, list(range(10000)), criterion, lr=0.001)]\n",
    "\n",
    "# train devices[0]\n",
    "for i in range(6):\n",
    "    train(i, center_list[0])\n",
    "    test(i, center_list[0])\n",
    "    center_list[0]['scheduler'].step()\n",
    "    \n",
    "# center_list[0]['fisher'] = fisher_matrix_diag(center_list[0], batch_size=fisher_batch_size)  # TODO: check another way\n",
    "center_list[0]['fisher'] = mas_matrix(center_list[0], batch_size=fisher_batch_size)  # TODO: check another way\n",
    "\n",
    "for i in range(1, number_center_devices):\n",
    "    center_list.append(copy.deepcopy(center_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center_device['fisher'].keys()\n",
    "\n",
    "# center_device['fisher']['fc1.weight']\n",
    "\n",
    "# plot_all_hist(center_device['fisher'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [1,2,3,4,5,6]\n",
    "# a[3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_net = ConvNet()\n",
    "# print(id(list(test_net.children())[0]))\n",
    "# print(id(test_net.conv1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test_net.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = 1\n",
    "local_epochs = 10\n",
    "num_devices = 50\n",
    "device_pct = 0.08\n",
    "data_pct = 0.02\n",
    "\n",
    "# data_idxs = iid_sampler(trainset, num_devices, data_pct)\n",
    "data_idxs = noniid_sampler(trainset, num_devices)\n",
    "\n",
    "\n",
    "num_device = int(3*10)\n",
    "fisher_lambda = 10\n",
    "l2_lambda = 1\n",
    "\n",
    "round_devices = [create_device(net, i, trainset, data_idxs[i], criterion, lr=0.001)\n",
    "                       for i in range(num_device)]\n",
    "\n",
    "number_group = 3\n",
    "group_size = len(round_devices) // number_group\n",
    "assert group_size*number_group == len(round_devices)\n",
    "\n",
    "\n",
    "\n",
    "# center_device: real global, train on global dataset\n",
    "# --- average_devices ---\n",
    "# center_list[0]: normal local training, normal average\n",
    "# center_list[1]: fisher local training, normal average\n",
    "# center_list[2]: l2     local training, normal average\n",
    "\n",
    "# center_list[3]: normal local training, fisher average\n",
    "# center_list[4]: fisher local training, fisher average\n",
    "# center_list[5]: l2     local training, fisher average\n",
    "# --- round_devices ---\n",
    "# round_devices[group_size*0 : group_size*1]: normal local training\n",
    "# round_devices[group_size*1 : group_size*2]: fisher local training\n",
    "# round_devices[group_size*2 : group_size*3]: l2 local training\n",
    "\n",
    "start_time = time.time()\n",
    "for round_num in range(rounds):\n",
    "    # Part 1.3: Implement getting devices for each round here\n",
    "    # round_devices = get_devices_for_round(devices, device_pct)\n",
    "    print('Round: ', round_num)\n",
    "    \n",
    "    # config local devices \n",
    "    for group_idx in range(number_group):\n",
    "        for device in round_devices[group_size*group_idx : group_size*(group_idx+1)]:\n",
    "            device['net'].load_state_dict(center_list[group_idx]['net'].state_dict())\n",
    "            device['optimizer'].zero_grad()\n",
    "            device['optimizer'].step()\n",
    "            \n",
    "            if group_idx in [1]:\n",
    "                device['criterion'] = get_fisher_criterion(\n",
    "                    criterion= criterion, \n",
    "                    fisher   = center_list[group_idx]['fisher'], \n",
    "                    model    = device['net'], \n",
    "                    model_old= center_list[group_idx]['net'], \n",
    "                    lamb     = fisher_lambda)\n",
    "                \n",
    "            if group_idx in [2]:\n",
    "                device['criterion'] = get_l2_criterion(\n",
    "                    criterion= criterion, \n",
    "                    model    = device['net'], \n",
    "                    model_old= center_list[group_idx]['net'], \n",
    "                    lamb     = l2_lambda)\n",
    "                \n",
    "#             if group_idx in [1, 3]:\n",
    "#                 freeze_idx = [2, 4]\n",
    "#                 for layer_idx, layer in enumerate(device['net'].children()):\n",
    "#                     if layer_idx in freeze_idx:\n",
    "#                         for n, p  in layer.named_parameters():\n",
    "#                             p.requires_grad = False\n",
    "    \n",
    "    for device_idx, device in enumerate(round_devices):\n",
    "        # if device_idx<=4:\n",
    "        #     continue\n",
    "        # test(-1, device)\n",
    "        for local_epoch in range(local_epochs):\n",
    "            train(local_epoch, device) \n",
    "            \n",
    "    # naive average\n",
    "    w_navg_nreg  = average_weights(round_devices[group_size*0 : group_size*1])\n",
    "#     w_navg_frreg = average_weights(round_devices[group_size*1 : group_size*2])\n",
    "    w_navg_fireg = average_weights(round_devices[group_size*1 : group_size*2])\n",
    "#     w_navg_fifrreg = average_weights(round_devices[group_size*3 : group_size*4])\n",
    "    w_navg_proxreg = average_weights(round_devices[group_size*2 : group_size*3])\n",
    "\n",
    "    center_list[0]['net'].load_state_dict(w_navg_nreg)\n",
    "#     center_list[1]['net'].load_state_dict(w_navg_frreg)\n",
    "    center_list[1]['net'].load_state_dict(w_navg_fireg)\n",
    "#     center_list[3]['net'].load_state_dict(w_navg_fifrreg)\n",
    "    center_list[2]['net'].load_state_dict(w_navg_proxreg)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # compute fisher and fisher average\n",
    "    for device in round_devices:\n",
    "        device['criterion'] = criterion\n",
    "        # device['fisher'] = fisher_matrix_diag(device, batch_size=fisher_batch_size)\n",
    "        device['fisher'] = mas_matrix(device, batch_size=fisher_batch_size)\n",
    "\n",
    "    w_favg_nreg = fisher_average_weights(round_devices[group_size*0 : group_size*1])\n",
    "    w_favg_fireg = fisher_average_weights(round_devices[group_size*1 : group_size*2])\n",
    "    w_favg_proxnreg = fisher_average_weights(round_devices[group_size*2 : group_size*3])\n",
    "\n",
    "\n",
    "    center_list[3]['net'].load_state_dict(w_favg_nreg)\n",
    "    center_list[4]['net'].load_state_dict(w_favg_fireg)\n",
    "    center_list[5]['net'].load_state_dict(w_favg_proxnreg)\n",
    "\n",
    "    \n",
    "    # record in 'test_acc_tracker'\n",
    "    print('\\n' + 'v-'*10 + '  Round %d Center Test  ' % round_num + '-v'*10 )\n",
    "    for i in range(len(center_list)):\n",
    "        test(round_num, center_list[i])\n",
    "    print('^-'*10 + '  Round %d Center Test  ' % round_num + '-^'*10 )\n",
    "\n",
    "    print('\\n' + 'v-'*10 + '  Round %d Local Test  ' % round_num + '-v'*10 )\n",
    "    for i in range(len(round_devices)):\n",
    "        if i%10 ==0:\n",
    "            print('-'*30)\n",
    "        test(round_num, round_devices[i])\n",
    "    print('^-'*10 + '  Round %d Local Test  ' % round_num + '-^'*10 )\n",
    "\n",
    "    # for i in [2, 3]:\n",
    "    #    center_list[i]['fisher'] = fisher_matrix_diag(center_list[i], batch_size=fisher_batch_size)  # TODO: check another way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(round_devices[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(center_list[2]['test_acc_tracker'], '*-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_hist(state_dict, ax, title, iflog=True):\n",
    "    big_tensor = []\n",
    "    for key in state_dict.keys():\n",
    "        big_tensor.append(state_dict[key].flatten())\n",
    "    big_tensor = torch.cat(big_tensor)\n",
    "\n",
    "    # sns.distplot(np.log(abs(big_tensor.cpu().numpy())+1e-12), ax=ax)\n",
    "    if iflog:\n",
    "        ax.hist(np.log10(abs(big_tensor.cpu().numpy())+1e-8), bins=100)\n",
    "    else:\n",
    "        ax.hist(big_tensor.cpu().numpy(), bins=100)\n",
    "        \n",
    "    ax.xaxis.set_tick_params(labelbottom=True)\n",
    "    ax.set_title(title)\n",
    "    # ax.set_xlabel('log(F_i)')\n",
    "    print('Name: %s, Max: %f, Min: %f' % (title, \n",
    "                                          abs(big_tensor.cpu().numpy()).max(),\n",
    "                                          abs(big_tensor.cpu().numpy()).min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,8])\n",
    "for i in range(10, 30):\n",
    "    if i in list(range(10, 20)):\n",
    "        ls = '-'\n",
    "    elif i in list(range(20, 30)):\n",
    "        ls = '-.'\n",
    "    else:\n",
    "        ls = '--'\n",
    "    plt.plot(round_devices[i]['train_fisher_loss_tracker'], label=str(i), linewidth=2, linestyle=ls)\n",
    "    \n",
    "plt.ylim([0,0.6])\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,8])\n",
    "for i in range(10, 30):\n",
    "    if i in list(range(10, 20)):\n",
    "        ls = '-'\n",
    "    elif i in list(range(20, 30)):\n",
    "        ls = '-.'\n",
    "    else:\n",
    "        ls = '--'\n",
    "    plt.plot(round_devices[i]['train_fisher_loss_tracker'], label=str(i), linewidth=2, linestyle=ls)\n",
    "    \n",
    "plt.ylim([0,0.02])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(center_list), 1,  figsize=(15,50), sharex=True)\n",
    "for i in range(len(center_list)):\n",
    "    plot_all_hist(center_list[i]['fisher'], ax[i], str(i), True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(round_devices), 1,  figsize=(15,80), sharex=True)\n",
    "for i in range(len(round_devices)):\n",
    "    plot_all_hist(round_devices[i]['fisher'], ax[i], str(i), True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=[24,8])\n",
    "# for i in [6,7,8,9,10,11]:\n",
    "#     p = ax1.plot(devices[i]['train_acc_tracker'], label='w/ fisher reg' if i <=8 else 'w/o fisher reg', linewidth=3, linestyle='dashed' if i<=8 else 'solid')\n",
    "#     ax2.plot([devices[i]['test_acc_tracker'][-1]]*len(devices[i]['train_acc_tracker']), label='w/ fisher reg' if i <=8 else 'w/o fisher reg', color=p[0].get_color(), linewidth=3, linestyle='dashed' if i<=8 else 'solid')\n",
    "    \n",
    "# ax1.plot([center_device['test_acc_tracker'][-1]]*len(devices[i]['train_acc_tracker']),'v', label='start from')\n",
    "# ax1.legend()\n",
    "# ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jovian\n",
    "# jovian.commit(filename='lab-tworound.ipynb', message='ID:3 version 2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
